{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.459651500Z",
     "start_time": "2025-01-07T23:14:46.170603900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM, TaskType, PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging, pipeline\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "import pandas as pd\n",
    "from textwrap import dedent\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "from metrics import  calculate_metrics, calculate_metrics2, calc_mets_my\n",
    "import gc\n",
    "import sys\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROJECT = \"Llama3-8B-QLora-FineTune-Omni\"\n",
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.472646700Z",
     "start_time": "2025-01-07T23:15:42.461648500Z"
    }
   },
   "id": "bf47d316a99ce8ac",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.849645800Z",
     "start_time": "2025-01-07T23:15:42.469648600Z"
    }
   },
   "id": "99bb3413d54b0eba",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.862643Z",
     "start_time": "2025-01-07T23:15:42.852640700Z"
    }
   },
   "id": "cc307c8e43a260d",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_distance(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        Distance: {row['distance']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "        Distance: {i['distance']}\n",
    "        Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions and the geographic distance between them is provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.874645900Z",
     "start_time": "2025-01-07T23:15:42.861645200Z"
    }
   },
   "id": "cd8b12926e356d69",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_distance(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        Distance: {row['distance']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "        Distance: {i['distance']}\n",
    "        Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions and the geographic distance between them is provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.883639800Z",
     "start_time": "2025-01-07T23:15:42.869650900Z"
    }
   },
   "id": "6f618e3b4dba82e0",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_att_val(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "       Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.884643400Z",
     "start_time": "2025-01-07T23:15:42.876644200Z"
    }
   },
   "id": "b431160970ef5de7",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_att_val(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "        Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.899641100Z",
     "start_time": "2025-01-07T23:15:42.883639800Z"
    }
   },
   "id": "f688a8935b4cd1c0",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\my_data_distance\\\\auck\\\\', 'datasets\\\\my_data_distance\\\\hope\\\\', 'datasets\\\\my_data_distance\\\\norse\\\\','datasets\\\\my_data_distance\\\\north\\\\', 'datasets\\\\my_data_distance\\\\palm\\\\', 'datasets\\\\osm_fsq_distance\\\\edi\\\\', 'datasets\\\\osm_fsq_distance\\\\pit\\\\', 'datasets\\\\osm_fsq_distance\\\\sin\\\\', 'datasets\\\\osm_fsq_distance\\\\tor\\\\', 'datasets\\\\osm_yelp_distance\\\\edi\\\\', 'datasets\\\\osm_yelp_distance\\\\pit\\\\', 'datasets\\\\osm_yelp_distance\\\\sin\\\\', 'datasets\\\\osm_yelp_distance\\\\tor\\\\', 'datasets\\\\acheson_distance\\\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T23:15:42.900643700Z",
     "start_time": "2025-01-07T23:15:42.894639200Z"
    }
   },
   "id": "9075c82adc311355",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    # random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    \n",
    "    yes_class = train_data.filter(lambda x: x[\"answer\"] == \"Yes\")\n",
    "    no_class = train_data.filter(lambda x: x[\"answer\"] == \"No\")\n",
    "    \n",
    "    yes_samples = yes_class.shuffle(seed=42).select(range(2))\n",
    "    no_samples = no_class.shuffle(seed=42).select(range(2))\n",
    "    \n",
    "    random_samples = concatenate_datasets([yes_samples, no_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    test_prompts = [format_test_distance(x, random_samples) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size=10\n",
    "    results=[]\n",
    "    start_time_test = time.time()\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(test_prompts), batch_size):\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                # batch = test_prompts[i:i + batch_size]\n",
    "                # inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,padding=True).to(device='cuda')\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=256,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 1,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                # results.extend([x.strip() for x in prediction])\n",
    "                results.append(prediction.strip())\n",
    "\n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    \n",
    "    predictions = [x.split(\" \")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label == \"Yes\" else 0 if label == \"No\" else 2 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"Yes\" else 0 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calc_mets_my(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    try:\n",
    "        bin_mets = calculate_metrics(predictions, labels, 'binary')\n",
    "        print(bin_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('binary failed')\n",
    "        bin_mets = 'binary failed'\n",
    "        \n",
    "    try:\n",
    "        micro_mets = calculate_metrics(predictions, labels, 'micro')\n",
    "        print(micro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('micro failed')\n",
    "        micro_mets = 'micro failed'\n",
    "        \n",
    "    try:\n",
    "        macro_mets = calculate_metrics(predictions, labels, 'macro')\n",
    "        print(macro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('macro failed')\n",
    "        macro_mets = 'macro failed'\n",
    "        \n",
    "    with open(\"class_balanced_distance_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(bin_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(micro_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(macro_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c0621d001cf03b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\my_data_att_val\\\\auck\\\\', 'datasets\\\\my_data_att_val\\\\hope\\\\', 'datasets\\\\my_data_att_val\\\\norse\\\\','datasets\\\\my_data_att_val\\\\north\\\\', 'datasets\\\\my_data_att_val\\\\palm\\\\', 'datasets\\\\osm_fsq_att_val\\\\edi\\\\', 'datasets\\\\osm_fsq_att_val\\\\pit\\\\', 'datasets\\\\osm_fsq_att_val\\\\sin\\\\', 'datasets\\\\osm_fsq_att_val\\\\tor\\\\', 'datasets\\\\osm_yelp_att_val\\\\edi\\\\', 'datasets\\\\osm_yelp_att_val\\\\pit\\\\', 'datasets\\\\osm_yelp_att_val\\\\sin\\\\', 'datasets\\\\osm_yelp_att_val\\\\tor\\\\', 'datasets\\\\acheson_att_val\\\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-08T02:37:29.843707700Z",
     "start_time": "2025-01-08T02:37:29.836710400Z"
    }
   },
   "id": "312747137d77221b",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    # random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    \n",
    "    yes_class = train_data.filter(lambda x: x[\"answer\"] == \"Yes\")\n",
    "    no_class = train_data.filter(lambda x: x[\"answer\"] == \"No\")\n",
    "    \n",
    "    yes_samples = yes_class.shuffle(seed=42).select(range(2))\n",
    "    no_samples = no_class.shuffle(seed=42).select(range(2))\n",
    "    \n",
    "    random_samples = concatenate_datasets([yes_samples, no_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    test_prompts = [format_test_att_val(x, random_samples) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size=10\n",
    "    results=[]\n",
    "    start_time_test = time.time()\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(test_prompts), batch_size):\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                # batch = test_prompts[i:i + batch_size]\n",
    "                # inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,padding=True).to(device='cuda')\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=256,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 1,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                # results.extend([x.strip() for x in prediction])\n",
    "                results.append(prediction.strip())\n",
    "\n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    \n",
    "    predictions = [x.split(\" \")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label == \"Yes\" else 0 if label == \"No\" else 2 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"Yes\" else 0 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calc_mets_my(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    try:\n",
    "        bin_mets = calculate_metrics(predictions, labels, 'binary')\n",
    "        print(bin_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('binary failed')\n",
    "        bin_mets = 'binary failed'\n",
    "        \n",
    "    try:\n",
    "        micro_mets = calculate_metrics(predictions, labels, 'micro')\n",
    "        print(micro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('micro failed')\n",
    "        micro_mets = 'micro failed'\n",
    "        \n",
    "    try:\n",
    "        macro_mets = calculate_metrics(predictions, labels, 'macro')\n",
    "        print(macro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('macro failed')\n",
    "        macro_mets = 'macro failed'\n",
    "        \n",
    "    with open(\"class_balanced_att_val_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(bin_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(micro_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(macro_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31cb108e3e5b964a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\gtminer_distance\\\\mel\\\\', 'datasets\\gtminer_distance\\\\sea\\\\', 'datasets\\gtminer_distance\\\\sin\\\\', 'datasets\\\\gtminer_distance\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-08T05:46:54.590487600Z",
     "start_time": "2025-01-08T05:46:54.583487500Z"
    }
   },
   "id": "77d98d810c9331d4",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    # model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    # random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    \n",
    "    same_as_class = train_data.filter(lambda x: x[\"answer\"] == \"same_as\")\n",
    "    part_of_class = train_data.filter(lambda x: x[\"answer\"] == \"part_of\")\n",
    "    serves_class = train_data.filter(lambda x: x[\"answer\"] == \"serves\")\n",
    "    unknown_class = train_data.filter(lambda x: x[\"answer\"] == \"unknown\")\n",
    "    \n",
    "    same_as_samples = same_as_class.shuffle(seed=42).select(range(2))\n",
    "    part_of_samples = part_of_class.shuffle(seed=42).select(range(2))\n",
    "    serves_samples = serves_class.shuffle(seed=42).select(range(2))\n",
    "    unknown_samples = unknown_class.shuffle(seed=42).select(range(2))\n",
    "    \n",
    "    random_samples = concatenate_datasets([same_as_samples, part_of_samples, serves_samples, unknown_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    # random_samples = same_as_samples + part_of_samples + serves_samples + unknown_samples\n",
    "    \n",
    "    test_prompts = [format_test_gtminer_distance(x, random_samples) for x in dataset['test']]\n",
    "    # elif dataset_output_path_1 ==\"gtminer3\":\n",
    "    #     test_prompts = [format_test_gtminer3(x) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    results=[]\n",
    "    start_time_test = time.time()  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=300,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 2,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                results.append(prediction.strip())\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    # print(results)\n",
    "    predictions = [x.split(\":\")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calculate_metrics2(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    try:\n",
    "        bin_mets = calculate_metrics(predictions, labels, 'binary')\n",
    "        print(bin_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('binary failed')\n",
    "        bin_mets = 'binary failed'\n",
    "        \n",
    "    try:\n",
    "        micro_mets = calculate_metrics(predictions, labels, 'micro')\n",
    "        print(micro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('micro failed')\n",
    "        micro_mets = 'micro failed'\n",
    "        \n",
    "    try:\n",
    "        macro_mets = calculate_metrics(predictions, labels, 'macro')\n",
    "        print(macro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('macro failed')\n",
    "        macro_mets = 'macro failed'\n",
    "        \n",
    "    with open(\"gtminer_class_balanced_distance_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(bin_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(micro_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(macro_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5459f4572156ae9b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\gtminer_att_val\\\\mel\\\\', 'datasets\\gtminer_att_val\\\\sea\\\\', 'datasets\\gtminer_att_val\\\\sin\\\\', 'datasets\\\\gtminer_att_val\\\\tor\\\\']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-08T08:02:01.961273Z",
     "start_time": "2025-01-08T08:02:01.954276600Z"
    }
   },
   "id": "b85ac37cf03c5768",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "format_test_gtminer_att_val\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    # model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    # random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    \n",
    "    same_as_class = train_data.filter(lambda x: x[\"answer\"] == \"same_as\")\n",
    "    part_of_class = train_data.filter(lambda x: x[\"answer\"] == \"part_of\")\n",
    "    serves_class = train_data.filter(lambda x: x[\"answer\"] == \"serves\")\n",
    "    unknown_class = train_data.filter(lambda x: x[\"answer\"] == \"unknown\")\n",
    "    \n",
    "    same_as_samples = same_as_class.shuffle(seed=42).select(range(2))\n",
    "    part_of_samples = part_of_class.shuffle(seed=42).select(range(2))\n",
    "    serves_samples = serves_class.shuffle(seed=42).select(range(2))\n",
    "    unknown_samples = unknown_class.shuffle(seed=42).select(range(2))\n",
    "    \n",
    "    random_samples = concatenate_datasets([same_as_samples, part_of_samples, serves_samples, unknown_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    # random_samples = same_as_samples + part_of_samples + serves_samples + unknown_samples\n",
    "    \n",
    "    test_prompts = [format_test_gtminer_att_val(x, random_samples) for x in dataset['test']]\n",
    "    # elif dataset_output_path_1 ==\"gtminer3\":\n",
    "    #     test_prompts = [format_test_gtminer3(x) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    results=[]\n",
    "    start_time_test = time.time()  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=300,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 2,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                results.append(prediction.strip())\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    # print(results)\n",
    "    predictions = [x.split(\":\")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calculate_metrics2(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    try:\n",
    "        bin_mets = calculate_metrics(predictions, labels, 'binary')\n",
    "        print(bin_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('binary failed')\n",
    "        bin_mets = 'binary failed'\n",
    "        \n",
    "    try:\n",
    "        micro_mets = calculate_metrics(predictions, labels, 'micro')\n",
    "        print(micro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('micro failed')\n",
    "        micro_mets = 'micro failed'\n",
    "        \n",
    "    try:\n",
    "        macro_mets = calculate_metrics(predictions, labels, 'macro')\n",
    "        print(macro_mets)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('macro failed')\n",
    "        macro_mets = 'macro failed'\n",
    "        \n",
    "    with open(\"gtminer_class_balanced_att_val_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(bin_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(micro_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(macro_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "438810a5af7fda84",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec09221ba5c65675"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
