{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:14.506575900Z",
     "start_time": "2025-02-04T11:30:24.619810500Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM, TaskType, PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging, pipeline\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from textwrap import dedent\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import warnings\n",
    "from metrics import  calculate_metrics, calculate_metrics2, calc_mets_my\n",
    "import gc\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "PROJECT = \"Llama3-8B-QLora-FineTune-Omni\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:14.519577300Z",
     "start_time": "2025-02-04T11:31:14.508580600Z"
    }
   },
   "id": "bf47d316a99ce8ac",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.082572200Z",
     "start_time": "2025-02-04T11:31:14.515576Z"
    }
   },
   "id": "99bb3413d54b0eba",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.095572700Z",
     "start_time": "2025-02-04T11:31:15.087571800Z"
    }
   },
   "id": "cc307c8e43a260d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_distance(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        Distance: {row['distance']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "        Distance: {i['distance']}\n",
    "        Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions and the geographic distance between them is provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.106574200Z",
     "start_time": "2025-02-04T11:31:15.092573600Z"
    }
   },
   "id": "cd8b12926e356d69",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_distance(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        Distance: {row['distance']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "        Distance: {i['distance']}\n",
    "        Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions and the geographic distance between them is provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.117575500Z",
     "start_time": "2025-02-04T11:31:15.101573100Z"
    }
   },
   "id": "6f618e3b4dba82e0",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_att_val(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "       Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.118572400Z",
     "start_time": "2025-02-04T11:31:15.110579200Z"
    }
   },
   "id": "b431160970ef5de7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_att_val(row, examples):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "    \"\"\"\n",
    "    )\n",
    "    samples = \"\"\n",
    "    for i in examples:\n",
    "        samples=samples+ dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{i[\"e1\"]}'\n",
    "        Place 2: '{i[\"e2\"]}'\n",
    "        Answer: {i['answer']}\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    full_prompt = messages[0][\"content\"] + samples + prompt + \"Answer: \"\n",
    "    return full_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.132085800Z",
     "start_time": "2025-02-04T11:31:15.115571300Z"
    }
   },
   "id": "f688a8935b4cd1c0",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set sampling strategy. 'class_balanced' or 'random'\n",
    "SAMPLING='class_balanced'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.133104600Z",
     "start_time": "2025-02-04T11:31:15.124572900Z"
    }
   },
   "id": "66a3d9eed5771576",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\NZER_attribute_value_dist\\\\hope\\\\', \n",
    "                       'datasets\\\\NZER_attribute_value_dist\\\\norse\\\\',\n",
    "                       'datasets\\\\NZER_attribute_value_dist\\\\palm\\\\', \n",
    "                       'datasets\\\\NZER_attribute_value_dist\\\\north\\\\', \n",
    "                       'datasets\\\\NZER_attribute_value_dist\\\\auck\\\\',\n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value_dist\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value_dist\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value_dist\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value_dist\\\\tor\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value_dist\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value_dist\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value_dist\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value_dist\\\\tor\\\\', \n",
    "                       'datasets\\\\SGN_attribute_value_dist\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:15.158104200Z",
     "start_time": "2025-02-04T11:31:15.131572500Z"
    }
   },
   "id": "9075c82adc311355",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'NZER_attribute_value_dist', 'hope', '']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'D:/omniLLM\\datasets\\NZER_attribute_value_dist\\hope\\train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m      7\u001B[0m dataset_output_path_1, dataset_output_path_2 \u001B[38;5;241m=\u001B[39m dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m], dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m----> 9\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuccessfully loaded dataset.......\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     16\u001B[0m     MODEL_NAME,\n\u001B[0;32m     17\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     22\u001B[0m )\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:2129\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2124\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2125\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2126\u001B[0m )\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2129\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2130\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2131\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2132\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2133\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2134\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2135\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2136\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2137\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2138\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2139\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2140\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2141\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2142\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2143\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2144\u001B[0m )\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1849\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1847\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1848\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1849\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1858\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_default_config_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_require_default_config_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_custom_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1862\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1558\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1541\u001B[0m \u001B[38;5;66;03m# We have several ways to get a dataset builder:\u001B[39;00m\n\u001B[0;32m   1542\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;66;03m# - if path is the name of a packaged dataset module\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1555\u001B[0m \n\u001B[0;32m   1556\u001B[0m \u001B[38;5;66;03m# Try packaged\u001B[39;00m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES:\n\u001B[1;32m-> 1558\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPackagedDatasetModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1563\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;66;03m# Try locally\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mendswith(filename):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:944\u001B[0m, in \u001B[0;36mPackagedDatasetModuleFactory.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    938\u001B[0m base_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mexpanduser()\u001B[38;5;241m.\u001B[39mresolve()\u001B[38;5;241m.\u001B[39mas_posix()\n\u001B[0;32m    939\u001B[0m patterns \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    940\u001B[0m     sanitize_patterns(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files)\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m get_data_patterns(base_path, download_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_config)\n\u001B[0;32m    943\u001B[0m )\n\u001B[1;32m--> 944\u001B[0m data_files \u001B[38;5;241m=\u001B[39m \u001B[43mDataFilesDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m supports_metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m _MODULE_SUPPORTS_METADATA\n\u001B[0;32m    950\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m supports_metadata \u001B[38;5;129;01mand\u001B[39;00m patterns \u001B[38;5;241m!=\u001B[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:721\u001B[0m, in \u001B[0;36mDataFilesDict.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    716\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m()\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, patterns_for_key \u001B[38;5;129;01min\u001B[39;00m patterns\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    718\u001B[0m     out[key] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    719\u001B[0m         patterns_for_key\n\u001B[0;32m    720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(patterns_for_key, DataFilesList)\n\u001B[1;32m--> 721\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mDataFilesList\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatterns_for_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    727\u001B[0m     )\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:624\u001B[0m, in \u001B[0;36mDataFilesList.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m patterns:\n\u001B[0;32m    622\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m         data_files\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m--> 624\u001B[0m             \u001B[43mresolve_pattern\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m                \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m         )\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_magic(pattern):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:411\u001B[0m, in \u001B[0;36mresolve_pattern\u001B[1;34m(pattern, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allowed_extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    410\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with any supported extension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(allowed_extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(error_msg)\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Unable to find 'D:/omniLLM\\datasets\\NZER_attribute_value_dist\\hope\\train.json'"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    if SAMPLING=='random':\n",
    "        random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    else:\n",
    "        yes_class = train_data.filter(lambda x: x[\"answer\"] == \"Yes\")\n",
    "        no_class = train_data.filter(lambda x: x[\"answer\"] == \"No\")\n",
    "        \n",
    "        yes_samples = yes_class.shuffle(seed=42).select(range(2))\n",
    "        no_samples = no_class.shuffle(seed=42).select(range(2))\n",
    "        \n",
    "        random_samples = concatenate_datasets([yes_samples, no_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    test_prompts = [format_test_distance(x, random_samples) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size=10\n",
    "    results=[]\n",
    "    start_time_test = time.time()\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(test_prompts), batch_size):\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                # batch = test_prompts[i:i + batch_size]\n",
    "                # inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,padding=True).to(device='cuda')\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=256,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 1,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                # results.extend([x.strip() for x in prediction])\n",
    "                results.append(prediction.strip())\n",
    "\n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    \n",
    "    predictions = [x.split(\" \")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label == \"Yes\" else 0 if label == \"No\" else 2 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"Yes\" else 0 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calc_mets_my(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    \n",
    "    results_logs = \"logs\\\\\"+SAMPLING+\"_few_shot_ER_attribute_value_distance_results.txt\"    \n",
    "    with open(results_logs, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:24.634224200Z",
     "start_time": "2025-02-04T11:31:22.623230300Z"
    }
   },
   "id": "33c0621d001cf03b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\NZER_attribute_value\\\\hope\\\\', \n",
    "                       'datasets\\\\NZER_attribute_value\\\\norse\\\\',\n",
    "                       'datasets\\\\NZER_attribute_value\\\\palm\\\\', \n",
    "                       'datasets\\\\NZER_attribute_value\\\\north\\\\', \n",
    "                       'datasets\\\\NZER_attribute_value\\\\auck\\\\',\n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_attribute_value\\\\tor\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_attribute_value\\\\tor\\\\', \n",
    "                       'datasets\\\\SGN_attribute_value\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:26.742210500Z",
     "start_time": "2025-02-04T11:31:26.735207800Z"
    }
   },
   "id": "312747137d77221b",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'NZER_attribute_value', 'hope', '']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'D:/omniLLM\\datasets\\NZER_attribute_value\\hope\\train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m      7\u001B[0m dataset_output_path_1, dataset_output_path_2 \u001B[38;5;241m=\u001B[39m dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m], dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m----> 9\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuccessfully loaded dataset.......\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     15\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     16\u001B[0m     MODEL_NAME,\n\u001B[0;32m     17\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     22\u001B[0m )\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:2129\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2124\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2125\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2126\u001B[0m )\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2129\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2130\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2131\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2132\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2133\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2134\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2135\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2136\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2137\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2138\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2139\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2140\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2141\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2142\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2143\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2144\u001B[0m )\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1849\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1847\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1848\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1849\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1858\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_default_config_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_require_default_config_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_custom_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1862\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1558\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1541\u001B[0m \u001B[38;5;66;03m# We have several ways to get a dataset builder:\u001B[39;00m\n\u001B[0;32m   1542\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;66;03m# - if path is the name of a packaged dataset module\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1555\u001B[0m \n\u001B[0;32m   1556\u001B[0m \u001B[38;5;66;03m# Try packaged\u001B[39;00m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES:\n\u001B[1;32m-> 1558\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPackagedDatasetModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1563\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;66;03m# Try locally\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mendswith(filename):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:944\u001B[0m, in \u001B[0;36mPackagedDatasetModuleFactory.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    938\u001B[0m base_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mexpanduser()\u001B[38;5;241m.\u001B[39mresolve()\u001B[38;5;241m.\u001B[39mas_posix()\n\u001B[0;32m    939\u001B[0m patterns \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    940\u001B[0m     sanitize_patterns(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files)\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m get_data_patterns(base_path, download_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_config)\n\u001B[0;32m    943\u001B[0m )\n\u001B[1;32m--> 944\u001B[0m data_files \u001B[38;5;241m=\u001B[39m \u001B[43mDataFilesDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m supports_metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m _MODULE_SUPPORTS_METADATA\n\u001B[0;32m    950\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m supports_metadata \u001B[38;5;129;01mand\u001B[39;00m patterns \u001B[38;5;241m!=\u001B[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:721\u001B[0m, in \u001B[0;36mDataFilesDict.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    716\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m()\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, patterns_for_key \u001B[38;5;129;01min\u001B[39;00m patterns\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    718\u001B[0m     out[key] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    719\u001B[0m         patterns_for_key\n\u001B[0;32m    720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(patterns_for_key, DataFilesList)\n\u001B[1;32m--> 721\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mDataFilesList\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatterns_for_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    727\u001B[0m     )\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:624\u001B[0m, in \u001B[0;36mDataFilesList.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m patterns:\n\u001B[0;32m    622\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m         data_files\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m--> 624\u001B[0m             \u001B[43mresolve_pattern\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m                \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m         )\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_magic(pattern):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:411\u001B[0m, in \u001B[0;36mresolve_pattern\u001B[1;34m(pattern, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allowed_extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    410\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with any supported extension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(allowed_extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(error_msg)\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Unable to find 'D:/omniLLM\\datasets\\NZER_attribute_value\\hope\\train.json'"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    if SAMPLING=='random':\n",
    "        random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    else:\n",
    "        yes_class = train_data.filter(lambda x: x[\"answer\"] == \"Yes\")\n",
    "        no_class = train_data.filter(lambda x: x[\"answer\"] == \"No\")\n",
    "        \n",
    "        yes_samples = yes_class.shuffle(seed=42).select(range(2))\n",
    "        no_samples = no_class.shuffle(seed=42).select(range(2))\n",
    "        \n",
    "        random_samples = concatenate_datasets([yes_samples, no_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    test_prompts = [format_test_att_val(x, random_samples) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size=10\n",
    "    results=[]\n",
    "    start_time_test = time.time()\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(test_prompts), batch_size):\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                # batch = test_prompts[i:i + batch_size]\n",
    "                # inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,padding=True).to(device='cuda')\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=256,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 1,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                # results.extend([x.strip() for x in prediction])\n",
    "                results.append(prediction.strip())\n",
    "\n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    \n",
    "    predictions = [x.split(\" \")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label == \"Yes\" else 0 if label == \"No\" else 2 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"Yes\" else 0 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calc_mets_my(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "\n",
    "        \n",
    "    with open(\"class_balanced_att_val_results.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:28.239199500Z",
     "start_time": "2025-02-04T11:31:27.861196600Z"
    }
   },
   "id": "31cb108e3e5b964a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set sampling strategy. 'class_balanced' or 'random'\n",
    "SAMPLING='class_balanced'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:29.637187800Z",
     "start_time": "2025-02-04T11:31:29.621188600Z"
    }
   },
   "id": "8afca5f8c7548038",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\GTMD_attribute_value_dist\\\\mel\\\\', \n",
    "                       'datasets\\\\GTMD_attribute_value_dist\\\\sea\\\\', \n",
    "                       'datasets\\\\GTMD_attribute_value_dist\\\\sin\\\\',\n",
    "                       'datasets\\\\GTMD_attribute_value_dist\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:30.259719700Z",
     "start_time": "2025-02-04T11:31:30.249187200Z"
    }
   },
   "id": "77d98d810c9331d4",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'GTMD_attribute_value_dist', 'mel', '']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'D:/omniLLM\\datasets\\GTMD_attribute_value_dist\\mel\\train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m      9\u001B[0m dataset_output_path_1, dataset_output_path_2 \u001B[38;5;241m=\u001B[39m dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m], dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m---> 11\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuccessfully loaded dataset.......\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     17\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     18\u001B[0m     MODEL_NAME,\n\u001B[0;32m     19\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     24\u001B[0m )\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:2129\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2124\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2125\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2126\u001B[0m )\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2129\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2130\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2131\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2132\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2133\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2134\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2135\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2136\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2137\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2138\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2139\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2140\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2141\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2142\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2143\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2144\u001B[0m )\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1849\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1847\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1848\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1849\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1858\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_default_config_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_require_default_config_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_custom_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1862\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1558\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1541\u001B[0m \u001B[38;5;66;03m# We have several ways to get a dataset builder:\u001B[39;00m\n\u001B[0;32m   1542\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;66;03m# - if path is the name of a packaged dataset module\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1555\u001B[0m \n\u001B[0;32m   1556\u001B[0m \u001B[38;5;66;03m# Try packaged\u001B[39;00m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES:\n\u001B[1;32m-> 1558\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPackagedDatasetModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1563\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;66;03m# Try locally\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mendswith(filename):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:944\u001B[0m, in \u001B[0;36mPackagedDatasetModuleFactory.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    938\u001B[0m base_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mexpanduser()\u001B[38;5;241m.\u001B[39mresolve()\u001B[38;5;241m.\u001B[39mas_posix()\n\u001B[0;32m    939\u001B[0m patterns \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    940\u001B[0m     sanitize_patterns(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files)\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m get_data_patterns(base_path, download_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_config)\n\u001B[0;32m    943\u001B[0m )\n\u001B[1;32m--> 944\u001B[0m data_files \u001B[38;5;241m=\u001B[39m \u001B[43mDataFilesDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m supports_metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m _MODULE_SUPPORTS_METADATA\n\u001B[0;32m    950\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m supports_metadata \u001B[38;5;129;01mand\u001B[39;00m patterns \u001B[38;5;241m!=\u001B[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:721\u001B[0m, in \u001B[0;36mDataFilesDict.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    716\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m()\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, patterns_for_key \u001B[38;5;129;01min\u001B[39;00m patterns\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    718\u001B[0m     out[key] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    719\u001B[0m         patterns_for_key\n\u001B[0;32m    720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(patterns_for_key, DataFilesList)\n\u001B[1;32m--> 721\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mDataFilesList\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatterns_for_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    727\u001B[0m     )\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:624\u001B[0m, in \u001B[0;36mDataFilesList.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m patterns:\n\u001B[0;32m    622\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m         data_files\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m--> 624\u001B[0m             \u001B[43mresolve_pattern\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m                \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m         )\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_magic(pattern):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:411\u001B[0m, in \u001B[0;36mresolve_pattern\u001B[1;34m(pattern, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allowed_extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    410\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with any supported extension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(allowed_extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(error_msg)\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Unable to find 'D:/omniLLM\\datasets\\GTMD_attribute_value_dist\\mel\\train.json'"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    # model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    # random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    \n",
    "    same_as_class = train_data.filter(lambda x: x[\"answer\"] == \"same_as\")\n",
    "    part_of_class = train_data.filter(lambda x: x[\"answer\"] == \"part_of\")\n",
    "    serves_class = train_data.filter(lambda x: x[\"answer\"] == \"serves\")\n",
    "    unknown_class = train_data.filter(lambda x: x[\"answer\"] == \"unknown\")\n",
    "    \n",
    "    same_as_samples = same_as_class.shuffle(seed=42).select(range(2))\n",
    "    part_of_samples = part_of_class.shuffle(seed=42).select(range(2))\n",
    "    serves_samples = serves_class.shuffle(seed=42).select(range(2))\n",
    "    unknown_samples = unknown_class.shuffle(seed=42).select(range(2))\n",
    "    \n",
    "    random_samples = concatenate_datasets([same_as_samples, part_of_samples, serves_samples, unknown_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "    \n",
    "    # random_samples = same_as_samples + part_of_samples + serves_samples + unknown_samples\n",
    "    \n",
    "    test_prompts = [format_test_gtminer_distance(x, random_samples) for x in dataset['test']]\n",
    "    # elif dataset_output_path_1 ==\"gtminer3\":\n",
    "    #     test_prompts = [format_test_gtminer3(x) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    results=[]\n",
    "    start_time_test = time.time()  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=300,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 2,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                results.append(prediction.strip())\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    # print(results)\n",
    "    predictions = [x.split(\":\")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calculate_metrics2(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "\n",
    "    \n",
    "    results_logs = \"logs\\\\GTMD_\"+SAMPLING+\"_few_shot_attribute_value_results.txt\"\n",
    "    with open(results_logs, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(bin_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(micro_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(macro_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:31.471248500Z",
     "start_time": "2025-02-04T11:31:31.076250Z"
    }
   },
   "id": "5459f4572156ae9b",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\GTMD_attribute_value\\\\mel\\\\', \n",
    "                       'datasets\\\\GTMD_attribute_value\\\\sea\\\\', \n",
    "                       'datasets\\\\GTMD_attribute_value\\\\sin\\\\',\n",
    "                       'datasets\\\\GTMD_attribute_value\\\\tor\\\\']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-04T11:31:31.468250200Z"
    }
   },
   "id": "b85ac37cf03c5768",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'GTMD_attribute_value_dist', 'mel', '']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'D:/omniLLM\\datasets\\GTMD_attribute_value_dist\\mel\\train.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     10\u001B[0m dataset_output_path_1, dataset_output_path_2 \u001B[38;5;241m=\u001B[39m dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m], dataset_folder\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m---> 12\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjson\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalid.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_folder\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuccessfully loaded dataset.......\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     19\u001B[0m     MODEL_NAME,\n\u001B[0;32m     20\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     25\u001B[0m )\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:2129\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2124\u001B[0m verification_mode \u001B[38;5;241m=\u001B[39m VerificationMode(\n\u001B[0;32m   2125\u001B[0m     (verification_mode \u001B[38;5;129;01mor\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mBASIC_CHECKS) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m save_infos \u001B[38;5;28;01melse\u001B[39;00m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS\n\u001B[0;32m   2126\u001B[0m )\n\u001B[0;32m   2128\u001B[0m \u001B[38;5;66;03m# Create a dataset builder\u001B[39;00m\n\u001B[1;32m-> 2129\u001B[0m builder_instance \u001B[38;5;241m=\u001B[39m load_dataset_builder(\n\u001B[0;32m   2130\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   2131\u001B[0m     name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2132\u001B[0m     data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m   2133\u001B[0m     data_files\u001B[38;5;241m=\u001B[39mdata_files,\n\u001B[0;32m   2134\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   2135\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m   2136\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config,\n\u001B[0;32m   2137\u001B[0m     download_mode\u001B[38;5;241m=\u001B[39mdownload_mode,\n\u001B[0;32m   2138\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m   2139\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m   2140\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m   2141\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[0;32m   2142\u001B[0m     _require_default_config_name\u001B[38;5;241m=\u001B[39mname \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2143\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig_kwargs,\n\u001B[0;32m   2144\u001B[0m )\n\u001B[0;32m   2146\u001B[0m \u001B[38;5;66;03m# Return iterable dataset in case of streaming\u001B[39;00m\n\u001B[0;32m   2147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m streaming:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1849\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001B[0m\n\u001B[0;32m   1847\u001B[0m     download_config \u001B[38;5;241m=\u001B[39m download_config\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m download_config \u001B[38;5;28;01melse\u001B[39;00m DownloadConfig()\n\u001B[0;32m   1848\u001B[0m     download_config\u001B[38;5;241m.\u001B[39mstorage_options\u001B[38;5;241m.\u001B[39mupdate(storage_options)\n\u001B[1;32m-> 1849\u001B[0m dataset_module \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_module_factory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1858\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_default_config_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_require_default_config_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_require_custom_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[38;5;66;03m# Get dataset builder class from the processing script\u001B[39;00m\n\u001B[0;32m   1862\u001B[0m builder_kwargs \u001B[38;5;241m=\u001B[39m dataset_module\u001B[38;5;241m.\u001B[39mbuilder_kwargs\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:1558\u001B[0m, in \u001B[0;36mdataset_module_factory\u001B[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001B[0m\n\u001B[0;32m   1541\u001B[0m \u001B[38;5;66;03m# We have several ways to get a dataset builder:\u001B[39;00m\n\u001B[0;32m   1542\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;66;03m# - if path is the name of a packaged dataset module\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1555\u001B[0m \n\u001B[0;32m   1556\u001B[0m \u001B[38;5;66;03m# Try packaged\u001B[39;00m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m _PACKAGED_DATASETS_MODULES:\n\u001B[1;32m-> 1558\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPackagedDatasetModuleFactory\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1562\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1563\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;66;03m# Try locally\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mendswith(filename):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\load.py:944\u001B[0m, in \u001B[0;36mPackagedDatasetModuleFactory.get_module\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    938\u001B[0m base_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mexpanduser()\u001B[38;5;241m.\u001B[39mresolve()\u001B[38;5;241m.\u001B[39mas_posix()\n\u001B[0;32m    939\u001B[0m patterns \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    940\u001B[0m     sanitize_patterns(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files)\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    942\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m get_data_patterns(base_path, download_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownload_config)\n\u001B[0;32m    943\u001B[0m )\n\u001B[1;32m--> 944\u001B[0m data_files \u001B[38;5;241m=\u001B[39m \u001B[43mDataFilesDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpatterns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    949\u001B[0m supports_metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m _MODULE_SUPPORTS_METADATA\n\u001B[0;32m    950\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m supports_metadata \u001B[38;5;129;01mand\u001B[39;00m patterns \u001B[38;5;241m!=\u001B[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:721\u001B[0m, in \u001B[0;36mDataFilesDict.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    716\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m()\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, patterns_for_key \u001B[38;5;129;01min\u001B[39;00m patterns\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    718\u001B[0m     out[key] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    719\u001B[0m         patterns_for_key\n\u001B[0;32m    720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(patterns_for_key, DataFilesList)\n\u001B[1;32m--> 721\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mDataFilesList\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_patterns\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    722\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatterns_for_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    724\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    725\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    726\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    727\u001B[0m     )\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:624\u001B[0m, in \u001B[0;36mDataFilesList.from_patterns\u001B[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m patterns:\n\u001B[0;32m    622\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    623\u001B[0m         data_files\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m--> 624\u001B[0m             \u001B[43mresolve_pattern\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m                \u001B[49m\u001B[43mallowed_extensions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallowed_extensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    630\u001B[0m         )\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_magic(pattern):\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\datasets\\data_files.py:411\u001B[0m, in \u001B[0;36mresolve_pattern\u001B[1;34m(pattern, base_path, allowed_extensions, download_config)\u001B[0m\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allowed_extensions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    410\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with any supported extension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(allowed_extensions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(error_msg)\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: Unable to find 'D:/omniLLM\\datasets\\GTMD_attribute_value_dist\\mel\\train.json'"
     ]
    }
   ],
   "source": [
    "format_test_gtminer_att_val\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    \n",
    "    if SAMPLING=='random':\n",
    "        random_samples = train_data.shuffle(seed=42).select(range(4))\n",
    "    else:\n",
    "    \n",
    "        same_as_class = train_data.filter(lambda x: x[\"answer\"] == \"same_as\")\n",
    "        part_of_class = train_data.filter(lambda x: x[\"answer\"] == \"part_of\")\n",
    "        serves_class = train_data.filter(lambda x: x[\"answer\"] == \"serves\")\n",
    "        unknown_class = train_data.filter(lambda x: x[\"answer\"] == \"unknown\")\n",
    "        \n",
    "        same_as_samples = same_as_class.shuffle(seed=42).select(range(2))\n",
    "        part_of_samples = part_of_class.shuffle(seed=42).select(range(2))\n",
    "        serves_samples = serves_class.shuffle(seed=42).select(range(2))\n",
    "        unknown_samples = unknown_class.shuffle(seed=42).select(range(2))\n",
    "        \n",
    "        random_samples = concatenate_datasets([same_as_samples, part_of_samples, serves_samples, unknown_samples])\n",
    "    \n",
    "    random_samples = random_samples.shuffle(seed=42)\n",
    "        \n",
    "    test_prompts = [format_test_gtminer_att_val(x, random_samples) for x in dataset['test']]\n",
    "    # elif dataset_output_path_1 ==\"gtminer3\":\n",
    "    #     test_prompts = [format_test_gtminer3(x) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    results=[]\n",
    "    start_time_test = time.time()  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=300,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 2,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                results.append(prediction.strip())\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    # print(results)\n",
    "    predictions = [x.split(\":\")[-1].strip() for x in results]\n",
    "    # predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calculate_metrics2(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "\n",
    "    \n",
    "    results_logs = \"logs\\\\GTMD_\"+SAMPLING+\"_few_shot_attribute_value_results.txt\"\n",
    "    with open(results_logs, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del dataset\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:31:32.566241800Z",
     "start_time": "2025-02-04T11:31:32.170242700Z"
    }
   },
   "id": "438810a5af7fda84",
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
