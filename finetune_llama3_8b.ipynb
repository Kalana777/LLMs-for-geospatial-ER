{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:16.956375600Z",
     "start_time": "2025-02-04T10:59:04.998367200Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM, TaskType, PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging, pipeline\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from textwrap import dedent\n",
    "from datasets import Dataset, load_dataset\n",
    "import warnings\n",
    "from metrics import  calculate_metrics, calculate_metrics2, calc_mets_my\n",
    "import gc\n",
    "import time\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROJECT = \"Llama3-8B-QLora-FineTune-Omni\"\n",
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:16.971375500Z",
     "start_time": "2025-02-04T10:59:16.958374100Z"
    }
   },
   "id": "8a2b7e3486d63697",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.345373300Z",
     "start_time": "2025-02-04T10:59:16.964378400Z"
    }
   },
   "id": "4bb0e1409fa5d683",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.359377500Z",
     "start_time": "2025-02-04T10:59:17.349372Z"
    }
   },
   "id": "37ec968f0cd5a103",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for  param in model.parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.362373600Z",
     "start_time": "2025-02-04T10:59:17.356374900Z"
    }
   },
   "id": "6389c74e9c9be294",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test(row: dict, scenario='fine-tune'):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer: \"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.384377200Z",
     "start_time": "2025-02-04T10:59:17.362373600Z"
    }
   },
   "id": "fe089cab062840d7",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_distance(row, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place1: '{row[\"e1\"]}'\n",
    "        Place2: '{row[\"e2\"]}'\n",
    "        Distance: {row['distance']}\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions and the geographic distance between them are provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.386380200Z",
     "start_time": "2025-02-04T10:59:17.372372800Z"
    }
   },
   "id": "dff2825e9fcbb229",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer(row: dict, scenario):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.388372600Z",
     "start_time": "2025-02-04T10:59:17.381372400Z"
    }
   },
   "id": "e1dc7cb63e95a4c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_simple(row: dict, scenario):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Predict the relation between them. Answer only with ‘same_as’, ‘part_of’, ‘serves’ or ‘unknown’.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.403374700Z",
     "start_time": "2025-02-04T10:59:17.388372600Z"
    }
   },
   "id": "afe8fe42e673a14e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_distance(row: dict, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    Distance: {row[\"distance\"]}\n",
    "    Answer only with: same_as, part_of, serves, unknown\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions and the geographic distance between them is provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.405373100Z",
     "start_time": "2025-02-04T10:59:17.395373Z"
    }
   },
   "id": "bce2b392b4660360",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer2(row: dict, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        Answer only with: same_as, part_of, serves, unknown\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer: \"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.435372500Z",
     "start_time": "2025-02-04T10:59:17.404374100Z"
    }
   },
   "id": "14bc1ec073594afb",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer3(row: dict, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    Answer only with: same-as, part-of, serves, unknown\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Predict the relation between the two places.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer: \"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.436376900Z",
     "start_time": "2025-02-04T10:59:17.413371600Z"
    }
   },
   "id": "3c6e94375e4762da",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run geospatial ER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3904961c7a43f299"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set to True if you want to save the finetuned model. Set to False otherwise.\n",
    "SAVE_FINE_TUNED_MODEL = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.529371400Z",
     "start_time": "2025-02-04T10:59:17.419371900Z"
    }
   },
   "id": "7eceda01b7303764",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot. Select between \"simple\", \"attribute_val\", \"plm\" and \"attribute_value_dist\"\n",
    "PROMPT_TO_USE = \"attribute_val\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:59:17.567370200Z",
     "start_time": "2025-02-04T10:59:17.429371700Z"
    }
   },
   "id": "4c33b0529628b84a",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\hope\\\\', \n",
    "                       'datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\norse\\\\',\n",
    "                       'datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\palm\\\\', \n",
    "                       'datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\north\\\\', \n",
    "                       'datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\auck\\\\',\n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\SGN_'+PROMPT_TO_USE+'\\\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T10:44:11.296358400Z",
     "start_time": "2025-02-04T10:44:11.277354100Z"
    }
   },
   "id": "f15d95676ed53885",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    \n",
    "    OUTPUT_DIR = \"experiments\\\\\"+ dataset_output_path_1 +\"\\\\\"+ dataset_output_path_2+\"\\\\\"\n",
    "    \n",
    "    os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "        \n",
    "    \n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=256,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"steps\",\n",
    "            warmup_ratio=0.1,\n",
    "            save_total_limit=0,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            save_safetensors=True,\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=256,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"no\",\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "        \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"starting training...........\")\n",
    "\n",
    "\n",
    "    start_time_train = time.time()\n",
    "    trainer.train()\n",
    "    end_time_train = time.time()\n",
    "    elapsed_time_train = end_time_train - start_time_train\n",
    "\n",
    "    print('training completed....')\n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "        print('model saved .........')\n",
    "    \n",
    "    if PROMPT_TO_USE ==\"attribute_value_dist\":\n",
    "        test_prompts = [format_test_distance(x) for x in dataset['test']]\n",
    "    else:\n",
    "        test_prompts = [format_test(x) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size=10\n",
    "    results=[]\n",
    "    start_time_test = time.time()\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(test_prompts), batch_size):\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                # batch = test_prompts[i:i + batch_size]\n",
    "                # inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,padding=True).to(device='cuda')\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=256,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 1,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                # results.extend([x.strip() for x in prediction])\n",
    "                results.append(prediction.strip())\n",
    "\n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    \n",
    "    # predictions = [x.split(\" \")[-1].strip() for x in results]\n",
    "    predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label == \"Yes\" else 0 if label == \"No\" else 2 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"Yes\" else 0 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calc_mets_my(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "\n",
    "    \n",
    "    results_logs = \"logs\\\\\"+PROMPT_TO_USE+\"_results.txt\"\n",
    "    with open(results_logs, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del dataset\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa40a52dc11414ed",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Geospatial relation classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b321b9055853f4de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set to True if you want to save the finetuned model. Set to False otherwise.\n",
    "SAVE_FINE_TUNED_MODEL = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:15:02.688092900Z",
     "start_time": "2025-02-04T11:15:02.682082400Z"
    }
   },
   "id": "c5d078cd71bff019",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot. Select between \"simple\", \"attribute_val\", \"plm\" and \"attribute_value_dist\"\n",
    "PROMPT_TO_USE = \"attribute_val\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:15:03.367083100Z",
     "start_time": "2025-02-04T11:15:03.350077Z"
    }
   },
   "id": "2f783704a9f933e2",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\mel\\\\', \n",
    "                       'datasets\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\sea\\\\', \n",
    "                       'datasets\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\sin\\\\',\n",
    "                       'datasets\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:15:03.976075100Z",
     "start_time": "2025-02-04T11:15:03.932075500Z"
    }
   },
   "id": "b68483f7cd0fc85e",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    \n",
    "    OUTPUT_DIR = \"experiments\\\\\"+ dataset_output_path_1 +\"\\\\\"+ dataset_output_path_2+\"\\\\\"\n",
    "    os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "\n",
    "    \n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=300,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"steps\",\n",
    "            warmup_ratio=0.1,\n",
    "            save_total_limit=0,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            save_safetensors=True,\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=300,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"no\",\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    # \n",
    "    print(\"starting training...........\")\n",
    "\n",
    "\n",
    "\n",
    "    start_time_train = time.time()\n",
    "    trainer.train()\n",
    "    end_time_train = time.time()\n",
    "    elapsed_time_train = end_time_train - start_time_train\n",
    "\n",
    "    print('training completed....')\n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "\n",
    "        print('model saved .........')\n",
    "    \n",
    "    # if dataset_output_path_1 ==\"gtminer\":\n",
    "    if PROMPT_TO_USE ==\"simple\":\n",
    "        test_prompts = [format_test_gtminer_simple(x) for x in dataset['test']]\n",
    "    elif PROMPT_TO_USE==\"attribute_value_dist\":\n",
    "        test_prompts = [format_test_gtminer_distance(x) for x in dataset['test']]\n",
    "    else:\n",
    "        test_prompts = [format_test_gtminer(x) for x in dataset['test']]\n",
    "    # elif dataset_output_path_1 ==\"gtminer3\":\n",
    "    #     test_prompts = [format_test_gtminer3(x) for x in dataset['test']]\n",
    "    \n",
    "    results=[]\n",
    "    start_time_test = time.time()  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=300,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 2,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                results.append(prediction.strip())\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    # print(results)\n",
    "    # predictions = [x.split(\":\")[-1].strip() for x in results]\n",
    "    predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calculate_metrics2(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "\n",
    "        \n",
    "    results_logs = \"logs\\\\GTMD_\"+PROMPT_TO_USE+\"_results.txt\"    \n",
    "    with open(results_logs, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        f.write(\"Train time: \"+str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(\"Test time: \" +str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del dataset\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c388bd36b26fd96",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained('experiments/my_data/auck')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, 'experiments/my_data/auck')\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cec7882aeabb40bd",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
