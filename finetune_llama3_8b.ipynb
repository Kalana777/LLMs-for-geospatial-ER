{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:52:37.919786800Z",
     "start_time": "2025-02-04T03:51:53.777604700Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM, TaskType, PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging, pipeline\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from textwrap import dedent\n",
    "from datasets import Dataset, load_dataset\n",
    "import warnings\n",
    "from metrics import  calculate_metrics, calculate_metrics2, calc_mets_my\n",
    "import gc\n",
    "import time\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROJECT = \"Llama3-8B-QLora-FineTune-Omni\"\n",
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:52:37.928790Z",
     "start_time": "2025-02-04T03:52:37.919786800Z"
    }
   },
   "id": "8a2b7e3486d63697",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:52:38.355796500Z",
     "start_time": "2025-02-04T03:52:37.928790Z"
    }
   },
   "id": "4bb0e1409fa5d683",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:52:38.374780600Z",
     "start_time": "2025-02-04T03:52:38.356785500Z"
    }
   },
   "id": "37ec968f0cd5a103",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for  param in model.parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:52:38.396311Z",
     "start_time": "2025-02-04T03:52:38.369785100Z"
    }
   },
   "id": "6389c74e9c9be294",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e3149a1e0c44f0aabea37bf53a2a46b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model........\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "print(\"loaded model........\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:11.509895300Z",
     "start_time": "2025-02-04T03:52:38.397309400Z"
    }
   },
   "id": "de6eed928449981a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1050939392 || all params: 4540600320 || trainable%: 23.145384264959926\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:11.520893200Z",
     "start_time": "2025-02-04T03:54:11.509895300Z"
    }
   },
   "id": "1d6fb3f83121ff19",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4540600320\n"
     ]
    }
   ],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(num_parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:11.587893Z",
     "start_time": "2025-02-04T03:54:11.517899300Z"
    }
   },
   "id": "18e0c689d289f1af",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.605882800Z",
     "start_time": "2025-02-04T03:54:11.532895200Z"
    }
   },
   "id": "f028555579bb554c",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167772160 || all params: 4708372480 || trainable%: 3.5632728870252848\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.617881800Z",
     "start_time": "2025-02-04T03:54:13.607880600Z"
    }
   },
   "id": "954369bb221d87da",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test(row: dict, scenario='fine-tune'):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer: \"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.682438600Z",
     "start_time": "2025-02-04T03:54:13.620881500Z"
    }
   },
   "id": "fe089cab062840d7",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_distance(row, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place1: '{row[\"e1\"]}'\n",
    "        Place2: '{row[\"e2\"]}'\n",
    "        Distance: {row['distance']}\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\":  \"Two place descriptions and the geographic distance between them are provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.705437Z",
     "start_time": "2025-02-04T03:54:13.661929800Z"
    }
   },
   "id": "dff2825e9fcbb229",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer(row: dict, scenario):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.756444Z",
     "start_time": "2025-02-04T03:54:13.695442600Z"
    }
   },
   "id": "e1dc7cb63e95a4c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_simple(row: dict, scenario):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Predict the relation between them. Answer only with ‘same_as’, ‘part_of’, ‘serves’ or ‘unknown’.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.782437500Z",
     "start_time": "2025-02-04T03:54:13.745453100Z"
    }
   },
   "id": "afe8fe42e673a14e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer_distance(row: dict, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    Distance: {row[\"distance\"]}\n",
    "    Answer only with: same_as, part_of, serves, unknown\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions and the geographic distance between them is provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer:\"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.794443900Z",
     "start_time": "2025-02-04T03:54:13.780438300Z"
    }
   },
   "id": "bce2b392b4660360",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer2(row: dict, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "        Place 1: '{row[\"e1\"]}'\n",
    "        Place 2: '{row[\"e2\"]}'\n",
    "        Answer only with: same_as, part_of, serves, unknown\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer: \"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.797439300Z",
     "start_time": "2025-02-04T03:54:13.788440200Z"
    }
   },
   "id": "14bc1ec073594afb",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_test_gtminer3(row: dict, scenario=\"fine-tune\"):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    Answer only with: same-as, part-of, serves, unknown\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Predict the relation between the two places.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    if scenario==\"zero\":\n",
    "        full_prompt = messages[0][\"content\"] + prompt + \"Answer: \"\n",
    "        return full_prompt\n",
    "    else:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.828444100Z",
     "start_time": "2025-02-04T03:54:13.796449100Z"
    }
   },
   "id": "3c6e94375e4762da",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set to True if you want to save the finetuned model. Set to False otherwise.\n",
    "SAVE_FINE_TUNED_MODEL = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.829439700Z",
     "start_time": "2025-02-04T03:54:13.804440800Z"
    }
   },
   "id": "7eceda01b7303764",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot. Select between \"simple\", \"attribute_val\", \"plm\" and \"attribute_value_dist\"\n",
    "PROMPT_TO_USE = \"attribute_val\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.830438200Z",
     "start_time": "2025-02-04T03:54:13.812440100Z"
    }
   },
   "id": "4c33b0529628b84a",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets2\\\\NZER_'+ PROMPT_TO_USE+ '\\\\hope\\\\', \n",
    "                       'datasets2\\\\NZER_'+ PROMPT_TO_USE+ '\\\\norse\\\\',\n",
    "                       'datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\palm\\\\', \n",
    "                       'datasets\\\\NZER_'+ PROMPT_TO_USE+ '\\\\north\\\\', \n",
    "                       'datasets2\\\\NZER_'+ PROMPT_TO_USE+ '\\\\auck\\\\',\n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ PROMPT_TO_USE+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ PROMPT_TO_USE+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\SGN_'+PROMPT_TO_USE+'\\\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:54:13.831437700Z",
     "start_time": "2025-02-04T03:54:13.818437400Z"
    }
   },
   "id": "f15d95676ed53885",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets2', 'NZER_attribute_val', 'hope', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f1fd251d0084807a19f2b4ce26d658d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00f9736e807f43408c780fa6531165a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b7223f6a0904ae5826486861bb5b312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded dataset.......\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bc5de569f654731a310cfa3d19a1cf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model........\n",
      "loaded tokenizer........\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Place1: 'name: McKay Creek type: stream latitude: -44.21667 longitude: 168.43333'\n",
      "Place2: 'name: Gipsy Creek type: Stream latitude: -44.022242 longitude: 168.684074'<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "No<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/13561 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1acd6b12cd8547a0966ceded8c5df1a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2906 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a43f0ee1c5c146cf98413618d9b4bccb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...........\n",
      "{'loss': 2.475, 'grad_norm': 0.5667222142219543, 'learning_rate': 0.0001, 'epoch': 0.017691287041132243}\n",
      "{'loss': 1.2191, 'grad_norm': 0.28920572996139526, 'learning_rate': 0.0001, 'epoch': 0.03538257408226449}\n",
      "{'loss': 1.0766, 'grad_norm': 0.23915338516235352, 'learning_rate': 0.0001, 'epoch': 0.05307386112339673}\n",
      "{'loss': 1.0342, 'grad_norm': 0.23277819156646729, 'learning_rate': 0.0001, 'epoch': 0.07076514816452897}\n",
      "{'loss': 0.9784, 'grad_norm': 0.22267933189868927, 'learning_rate': 0.0001, 'epoch': 0.08845643520566121}\n",
      "{'loss': 0.9375, 'grad_norm': 0.28909575939178467, 'learning_rate': 0.0001, 'epoch': 0.10614772224679346}\n",
      "{'loss': 0.8784, 'grad_norm': 0.45711883902549744, 'learning_rate': 0.0001, 'epoch': 0.1238390092879257}\n",
      "{'loss': 0.7825, 'grad_norm': 0.30580389499664307, 'learning_rate': 0.0001, 'epoch': 0.14153029632905795}\n",
      "{'loss': 0.7558, 'grad_norm': 0.28000837564468384, 'learning_rate': 0.0001, 'epoch': 0.15922158337019018}\n",
      "{'loss': 0.7222, 'grad_norm': 0.4044952094554901, 'learning_rate': 0.0001, 'epoch': 0.17691287041132242}\n",
      "{'loss': 0.7078, 'grad_norm': 0.3219354450702667, 'learning_rate': 0.0001, 'epoch': 0.19460415745245466}\n",
      "{'loss': 0.6706, 'grad_norm': 0.3677520751953125, 'learning_rate': 0.0001, 'epoch': 0.21229544449358692}\n",
      "{'loss': 0.6443, 'grad_norm': 0.4264160096645355, 'learning_rate': 0.0001, 'epoch': 0.22998673153471916}\n",
      "{'loss': 0.559, 'grad_norm': 0.4578710198402405, 'learning_rate': 0.0001, 'epoch': 0.2476780185758514}\n",
      "{'loss': 0.5061, 'grad_norm': 0.49342429637908936, 'learning_rate': 0.0001, 'epoch': 0.26536930561698363}\n",
      "{'loss': 0.4283, 'grad_norm': 0.6272316575050354, 'learning_rate': 0.0001, 'epoch': 0.2830605926581159}\n",
      "{'loss': 0.3936, 'grad_norm': 0.4913145899772644, 'learning_rate': 0.0001, 'epoch': 0.3007518796992481}\n",
      "{'loss': 0.3491, 'grad_norm': 0.5590912699699402, 'learning_rate': 0.0001, 'epoch': 0.31844316674038037}\n",
      "{'loss': 0.3137, 'grad_norm': 0.5646758079528809, 'learning_rate': 0.0001, 'epoch': 0.33613445378151263}\n",
      "{'loss': 0.2862, 'grad_norm': 0.438224732875824, 'learning_rate': 0.0001, 'epoch': 0.35382574082264484}\n",
      "{'loss': 0.266, 'grad_norm': 0.5022090673446655, 'learning_rate': 0.0001, 'epoch': 0.3715170278637771}\n",
      "{'loss': 0.2318, 'grad_norm': 0.5239420533180237, 'learning_rate': 0.0001, 'epoch': 0.3892083149049093}\n",
      "{'loss': 0.2076, 'grad_norm': 0.4355974793434143, 'learning_rate': 0.0001, 'epoch': 0.4068996019460416}\n",
      "{'loss': 0.2084, 'grad_norm': 0.45214223861694336, 'learning_rate': 0.0001, 'epoch': 0.42459088898717384}\n",
      "{'loss': 0.1924, 'grad_norm': 0.37286776304244995, 'learning_rate': 0.0001, 'epoch': 0.44228217602830605}\n",
      "{'loss': 0.1885, 'grad_norm': 0.3951541483402252, 'learning_rate': 0.0001, 'epoch': 0.4599734630694383}\n",
      "{'loss': 0.1749, 'grad_norm': 0.30088111758232117, 'learning_rate': 0.0001, 'epoch': 0.4776647501105705}\n",
      "{'loss': 0.1781, 'grad_norm': 0.3029181957244873, 'learning_rate': 0.0001, 'epoch': 0.4953560371517028}\n",
      "{'loss': 0.1664, 'grad_norm': 0.2891436219215393, 'learning_rate': 0.0001, 'epoch': 0.513047324192835}\n",
      "{'loss': 0.1652, 'grad_norm': 0.26779112219810486, 'learning_rate': 0.0001, 'epoch': 0.5307386112339673}\n",
      "{'loss': 0.161, 'grad_norm': 0.27274289727211, 'learning_rate': 0.0001, 'epoch': 0.5484298982750995}\n",
      "{'loss': 0.1557, 'grad_norm': 0.23564040660858154, 'learning_rate': 0.0001, 'epoch': 0.5661211853162318}\n",
      "{'loss': 0.1523, 'grad_norm': 0.26215219497680664, 'learning_rate': 0.0001, 'epoch': 0.583812472357364}\n",
      "{'loss': 0.1497, 'grad_norm': 0.25341281294822693, 'learning_rate': 0.0001, 'epoch': 0.6015037593984962}\n",
      "{'loss': 0.1512, 'grad_norm': 0.2298363596200943, 'learning_rate': 0.0001, 'epoch': 0.6191950464396285}\n",
      "{'loss': 0.1482, 'grad_norm': 0.20791466534137726, 'learning_rate': 0.0001, 'epoch': 0.6368863334807607}\n",
      "{'loss': 0.1484, 'grad_norm': 0.2400882989168167, 'learning_rate': 0.0001, 'epoch': 0.6545776205218929}\n",
      "{'loss': 0.1475, 'grad_norm': 0.23119916021823883, 'learning_rate': 0.0001, 'epoch': 0.6722689075630253}\n",
      "{'loss': 0.1464, 'grad_norm': 0.184625044465065, 'learning_rate': 0.0001, 'epoch': 0.6899601946041575}\n",
      "{'loss': 0.1435, 'grad_norm': 0.19501729309558868, 'learning_rate': 0.0001, 'epoch': 0.7076514816452897}\n",
      "{'loss': 0.1441, 'grad_norm': 0.15679368376731873, 'learning_rate': 0.0001, 'epoch': 0.7253427686864219}\n",
      "{'loss': 0.1455, 'grad_norm': 0.18798701465129852, 'learning_rate': 0.0001, 'epoch': 0.7430340557275542}\n",
      "{'loss': 0.1412, 'grad_norm': 0.21113212406635284, 'learning_rate': 0.0001, 'epoch': 0.7607253427686864}\n",
      "{'loss': 0.139, 'grad_norm': 0.1429373174905777, 'learning_rate': 0.0001, 'epoch': 0.7784166298098186}\n",
      "{'loss': 0.14, 'grad_norm': 0.17507502436637878, 'learning_rate': 0.0001, 'epoch': 0.796107916850951}\n",
      "{'loss': 0.1402, 'grad_norm': 0.22139841318130493, 'learning_rate': 0.0001, 'epoch': 0.8137992038920832}\n",
      "{'loss': 0.1382, 'grad_norm': 0.1659182608127594, 'learning_rate': 0.0001, 'epoch': 0.8314904909332154}\n",
      "{'loss': 0.1398, 'grad_norm': 0.17704758048057556, 'learning_rate': 0.0001, 'epoch': 0.8491817779743477}\n",
      "{'loss': 0.1373, 'grad_norm': 0.14693164825439453, 'learning_rate': 0.0001, 'epoch': 0.8668730650154799}\n",
      "{'loss': 0.1389, 'grad_norm': 0.15784555673599243, 'learning_rate': 0.0001, 'epoch': 0.8845643520566121}\n",
      "{'loss': 0.1387, 'grad_norm': 0.1641484498977661, 'learning_rate': 0.0001, 'epoch': 0.9022556390977443}\n",
      "{'loss': 0.1376, 'grad_norm': 0.16276559233665466, 'learning_rate': 0.0001, 'epoch': 0.9199469261388766}\n",
      "{'loss': 0.1351, 'grad_norm': 0.1397274136543274, 'learning_rate': 0.0001, 'epoch': 0.9376382131800088}\n",
      "{'loss': 0.139, 'grad_norm': 0.15882758796215057, 'learning_rate': 0.0001, 'epoch': 0.955329500221141}\n",
      "{'loss': 0.1366, 'grad_norm': 0.19287949800491333, 'learning_rate': 0.0001, 'epoch': 0.9730207872622734}\n",
      "{'loss': 0.1359, 'grad_norm': 0.14894266426563263, 'learning_rate': 0.0001, 'epoch': 0.9907120743034056}\n",
      "{'train_runtime': 2905.5794, 'train_samples_per_second': 4.667, 'train_steps_per_second': 0.194, 'train_loss': 0.3804881035754111, 'epoch': 0.9995577178239717}\n",
      "training completed....\n",
      "model saved .........\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Place 1: 'name: Silver Stream type: stream latitude: -44.04833 longitude: 168.67008'\n",
      "Place 2: 'name: Deep Dale type: Valley latitude: -44.025083 longitude: 168.672056'<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    \n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "        OUTPUT_DIR = \"experiments\\\\\"+ dataset_output_path_1 +\"\\\\\"+ dataset_output_path_2+\"\\\\\"\n",
    "    \n",
    "        os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "    \n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=256,\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"steps\",\n",
    "            warmup_ratio=0.1,\n",
    "            save_total_limit=0,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            save_safetensors=True,\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=256,\n",
    "            num_train_epochs=5,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"no\",\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "        \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"starting training...........\")\n",
    "\n",
    "\n",
    "    start_time_train = time.time()\n",
    "    trainer.train()\n",
    "    end_time_train = time.time()\n",
    "    elapsed_time_train = end_time_train - start_time_train\n",
    "\n",
    "    print('training completed....')\n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "        print('model saved .........')\n",
    "    \n",
    "    if PROMPT_TO_USE ==\"attribute_value_dist\":\n",
    "        test_prompts = [format_test_distance(x) for x in dataset['test']]\n",
    "    else:\n",
    "        test_prompts = [format_test(x) for x in dataset['test']]\n",
    "    \n",
    "    print(test_prompts[0])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size=10\n",
    "    results=[]\n",
    "    start_time_test = time.time()\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(test_prompts), batch_size):\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                # batch = test_prompts[i:i + batch_size]\n",
    "                # inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,padding=True).to(device='cuda')\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=256,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 1,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                # results.extend([x.strip() for x in prediction])\n",
    "                results.append(prediction.strip())\n",
    "\n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    \n",
    "    # predictions = [x.split(\" \")[-1].strip() for x in results]\n",
    "    predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label == \"Yes\" else 0 if label == \"No\" else 2 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"Yes\" else 0 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calc_mets_my(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    # try:\n",
    "    #     bin_mets = calculate_metrics(predictions, labels, 'binary')\n",
    "    #     print(bin_mets)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print('binary failed')\n",
    "    #     bin_mets = 'binary failed'\n",
    "    #     \n",
    "    # try:\n",
    "    #     micro_mets = calculate_metrics(predictions, labels, 'micro')\n",
    "    #     print(micro_mets)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print('micro failed')\n",
    "    #     micro_mets = 'micro failed'\n",
    "    #     \n",
    "    # try:\n",
    "    #     macro_mets = calculate_metrics(predictions, labels, 'macro')\n",
    "    #     print(macro_mets)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print('macro failed')\n",
    "    #     macro_mets = 'macro failed'\n",
    "    \n",
    "    results_logs = \"logs\\\\\"+PROMPT_TO_USE+\"_results.txt\"\n",
    "    with open(\"results_logs\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(bin_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(micro_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(macro_mets))\n",
    "        # f.write('\\n')\n",
    "        f.write(str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del dataset\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-02-04T03:54:13.824440Z"
    }
   },
   "id": "aa40a52dc11414ed",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set to True if you want to save the finetuned model. Set to False otherwise.\n",
    "SAVE_FINE_TUNED_MODEL = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T03:48:45.060987500Z",
     "start_time": "2025-02-04T03:48:45.059983800Z"
    }
   },
   "id": "c5d078cd71bff019"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot. Select between \"simple\", \"attribute_val\", \"plm\" and \"attribute_value_dist\"\n",
    "PROMPT_TO_USE = \"attribute_val\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f783704a9f933e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets2\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\mel\\\\', \n",
    "                       'datasets2\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\sea\\\\', \n",
    "                       'datasets2\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\sin\\\\',\n",
    "                       'datasets2\\\\GTMD_'+ PROMPT_TO_USE+ '\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b68483f7cd0fc85e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets', 'gtminer_plm', 'mel', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48afc27593c64c6cb692194af984d523"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86fc4a31e1ca47dbb9a6be6e861a3ee6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd88c97bb703436598e14c371c73d4c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded dataset.......\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "067813969c9b4f988bc34c31686a1784"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model........\n",
      "loaded tokenizer........\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Place 1: 'COL name VAL Department of Environment, Land, Water and Planning COL type VAL office COL address VAL 8 Nicholson Street 3002 COL latitude VAL -37.8083722 COL longitude VAL 144.9734604 '\n",
      "Place 2: 'COL name VAL Hotel Ovolo COL type VAL hotel COL address VAL 19 Little Bourke Street 3000 COL latitude VAL -37.8107508 COL longitude VAL 144.9719983 '\n",
      "Answer only with: same_as, part_of, serves, unknown<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "unknown<|eot_id|>\n",
      "testing completed........\n",
      "['datasets', 'gtminer_plm', 'mel', '']\n",
      "P: 0.4533  |  R: 0.7398  |  F1: 0.5621\n",
      "{'precision': 0.4533042053522665, 'recall': 0.7397504456327986, 'f1': 0.5621401964104301}\n",
      "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "binary failed\n",
      "{'Precision': 0.4551386623164764, 'Recall': 0.4551386623164764, 'F1 Score': 0.4551386623164764}\n",
      "{'Precision': 0.5225, 'Recall': 0.2835906340736077, 'F1 Score': 0.21398691256352936}\n",
      "['datasets', 'gtminer_plm', 'sea', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "807dd74e7d3047e7981d0ad29f3e52ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9aa4391405a44a35b95a0913da02226d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bdde17e52924c34866102cee4f066c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded dataset.......\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5632a7e0713f4e188b3bd16ec22e1033"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model........\n",
      "loaded tokenizer........\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Place 1: 'COL name VAL The Grilled Cheese Experience COL type VAL cafe COL address VAL 909 Pike Street 98101 COL latitude VAL 47.6123242 COL longitude VAL -122.3309093 '\n",
      "Place 2: 'COL name VAL Washington State Convention Center COL type VAL attraction COL address VAL 800 Convention Place 98101 COL latitude VAL 47.6117274 COL longitude VAL -122.3316528 '\n",
      "Answer only with: same_as, part_of, serves, unknown<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "part_of<|eot_id|>\n",
      "testing completed........\n",
      "['datasets', 'gtminer_plm', 'sea', '']\n",
      "P: 0.2899  |  R: 0.6962  |  F1: 0.4093\n",
      "{'precision': 0.2898828541001065, 'recall': 0.6961636828644501, 'f1': 0.4093233082706767}\n",
      "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "binary failed\n",
      "{'Precision': 0.2972403623341057, 'Recall': 0.2972403623341057, 'F1 Score': 0.2972403623341057}\n",
      "{'Precision': 0.521038890983744, 'Recall': 0.3211689533341233, 'F1 Score': 0.2221926608184136}\n",
      "['datasets', 'gtminer_plm', 'sin', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05728558075b4703ad64af59b0ea055b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77d99bdd03c24bd9a362c79863ede856"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfe0fd5100624a6b8671197c4812dd63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded dataset.......\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39227e1a53b7425e99835bcfc7d41001"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model........\n",
      "loaded tokenizer........\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Place 1: 'COL name VAL City Square Post Office COL type VAL post_office COL address VAL 180 Kitchener Road #B2-33 City Square 208539 COL latitude VAL 1.3109755 COL longitude VAL 103.8567196 '\n",
      "Place 2: 'COL name VAL Mustafa Centre COL type VAL Shopping Centers COL address VAL 145 Syed Alwi Road 207704 COL latitude VAL 1.3101243 COL longitude VAL 103.8553164 '\n",
      "Answer only with: same_as, part_of, serves, unknown<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "unknown<|eot_id|>\n",
      "testing completed........\n",
      "['datasets', 'gtminer_plm', 'sin', '']\n",
      "P: 0.3317  |  R: 0.6598  |  F1: 0.4414\n",
      "{'precision': 0.3316629302386918, 'recall': 0.6597586568730325, 'f1': 0.44142167617376044}\n",
      "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "binary failed\n",
      "{'Precision': 0.34742740703005603, 'Recall': 0.34742740703005603, 'F1 Score': 0.34742740703005603}\n",
      "{'Precision': 0.3800791940791522, 'Recall': 0.31811181452421367, 'F1 Score': 0.23024974461725892}\n",
      "['datasets', 'gtminer_plm', 'tor', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40fd07007ca94e5c8d2462f3010e64ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b84f79db9fde4d10bf4df50d17a19d8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c7cc5578bbc46c5a4e12341afd663fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded dataset.......\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3463b0246584f03a73cf5d691b96e21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model........\n",
      "loaded tokenizer........\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Place 1: 'COL name VAL First Copy COL type VAL copyshop COL address VAL 61 College Street COL latitude VAL 43.6605215 COL longitude VAL -79.386228 '\n",
      "Place 2: 'COL name VAL Medical Sciences Building COL type VAL Colleges & Universities; Education COL address VAL 1 King's College Cir M5S 1A8 COL latitude VAL 43.6607748 COL longitude VAL -79.3933729 '\n",
      "Answer only with: same_as, part_of, serves, unknown<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "unknown<|eot_id|>\n",
      "testing completed........\n",
      "['datasets', 'gtminer_plm', 'tor', '']\n",
      "P: 0.344  |  R: 0.7214  |  F1: 0.4659\n",
      "{'precision': 0.3440192346223202, 'recall': 0.7214285714285714, 'f1': 0.46587979921313255}\n",
      "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "binary failed\n",
      "{'Precision': 0.35522446579102135, 'Recall': 0.35522446579102135, 'F1 Score': 0.35522446579102135}\n",
      "{'Precision': 0.4195985408269891, 'Recall': 0.29651584934433334, 'F1 Score': 0.2085200195530726}\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    " \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    print(\"successfully loaded dataset.......\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded model........\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    print(\"loaded tokenizer........\")\n",
    "    PAD_TOKEN = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(dataset['train'][0]['text'])\n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "    \n",
    "        OUTPUT_DIR = \"experiments\\\\\"+ dataset_output_path_1 +\"\\\\\"+ dataset_output_path_2+\"\\\\\"\n",
    "    \n",
    "        os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "    \n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=300,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"steps\",\n",
    "            warmup_ratio=0.1,\n",
    "            save_total_limit=0,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            save_safetensors=True,\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        sft_config = SFTConfig(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=300,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=6,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            logging_steps=10,\n",
    "            learning_rate=1e-4,\n",
    "            bf16=True,\n",
    "            save_strategy=\"no\",\n",
    "            warmup_ratio=0.1,\n",
    "            lr_scheduler_type=\"constant\",\n",
    "            dataset_kwargs={\n",
    "                \"add_special_tokens\": False,  \n",
    "                \"append_concat_token\": False, \n",
    "            },\n",
    "        )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"valid\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    # \n",
    "    print(\"starting training...........\")\n",
    "\n",
    "\n",
    "\n",
    "    start_time_train = time.time()\n",
    "    trainer.train()\n",
    "    end_time_train = time.time()\n",
    "    elapsed_time_train = end_time_train - start_time_train\n",
    "\n",
    "    print('training completed....')\n",
    "    \n",
    "    if SAVE_FINE_TUNED_MODEL:\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "\n",
    "        print('model saved .........')\n",
    "    \n",
    "    # if dataset_output_path_1 ==\"gtminer\":\n",
    "    if PROMPT_TO_USE ==\"simple\":\n",
    "        test_prompts = [format_test_gtminer_simple(x) for x in dataset['test']]\n",
    "    elif PROMPT_TO_USE==\"attribute_value_dist\":\n",
    "        test_prompts = [format_test_gtminer_distance(x) for x in dataset['test']]\n",
    "    else:\n",
    "        test_prompts = [format_test_gtminer(x) for x in dataset['test']]\n",
    "    # elif dataset_output_path_1 ==\"gtminer3\":\n",
    "    #     test_prompts = [format_test_gtminer3(x) for x in dataset['test']]\n",
    "    \n",
    "    results=[]\n",
    "    start_time_test = time.time()  \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device='cuda')\n",
    "                # outputs = model.pipeline(inputs.input_ids)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids, \n",
    "                    max_length=300,  # Maximum length of the generated text\n",
    "                    max_new_tokens= 2,\n",
    "                    num_return_sequences=1,  # Number of sequences to generate\n",
    "                    no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "                    temperature=0.01,  # Controls randomness; lower is less random\n",
    "                    top_k=50,  # Top-k sampling\n",
    "                )\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "                results.append(prediction.strip())\n",
    "    \n",
    "    end_time_test = time.time()\n",
    "    elapsed_time_test = end_time_test - start_time_test\n",
    "    \n",
    "    print(\"testing completed........\")\n",
    "    # print(results)\n",
    "    # predictions = [x.split(\":\")[-1].strip() for x in results]\n",
    "    predictions = [x.split(\"\\n\")[-1].strip() for x in results]\n",
    "    \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    # print(predictions)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    # print(labels)\n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    \n",
    "    try:\n",
    "        my_mets = calculate_metrics2(predictions, labels)\n",
    "        print(my_mets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('my calc failed')\n",
    "        my_mets = 'my calc failed'\n",
    "    \n",
    "    # try:\n",
    "    #     bin_mets = calculate_metrics(predictions, labels, 'binary')\n",
    "    #     print(bin_mets)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print('binary failed')\n",
    "    #     bin_mets = 'binary failed'\n",
    "        \n",
    "    # try:\n",
    "    #     micro_mets = calculate_metrics(predictions, labels, 'micro')\n",
    "    #     print(micro_mets)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print('micro failed')\n",
    "    #     micro_mets = 'micro failed'\n",
    "    #     \n",
    "    # try:\n",
    "    #     macro_mets = calculate_metrics(predictions, labels, 'macro')\n",
    "    #     print(macro_mets)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print('macro failed')\n",
    "    #     macro_mets = 'macro failed'\n",
    "        \n",
    "        \n",
    "    results_logs = \"logs\\\\GTMD_\"+PROMPT_TO_USE+\"_results.txt\"    \n",
    "    with open(results_logs, \"a\", encoding='utf-8') as f:\n",
    "        f.write(str(dataset_output_path_1))\n",
    "        f.write(str(dataset_output_path_2))\n",
    "        f.write('\\n')\n",
    "        f.write(str(dataset['train'][0]['text']))\n",
    "        f.write('\\n')\n",
    "        f.write(str(results[0]))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write(str(my_mets))\n",
    "        f.write('\\n')\n",
    "        # f.write(str(bin_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(micro_mets))\n",
    "        # f.write('\\n')\n",
    "        # f.write(str(macro_mets))\n",
    "        # f.write('\\n')\n",
    "        f.write(\"Train time: \"+str(elapsed_time_train))\n",
    "        f.write('\\n')\n",
    "        f.write(\"Test time: \" +str(elapsed_time_test))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('********************************')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    del model  # Delete the model instance\n",
    "    del dataset\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T03:34:40.010831800Z",
     "start_time": "2025-01-09T02:10:29.560105600Z"
    }
   },
   "id": "3c388bd36b26fd96",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained('experiments/my_data/auck')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, 'experiments/my_data/auck')\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cec7882aeabb40bd",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
