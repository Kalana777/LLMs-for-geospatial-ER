{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:46:42.863015300Z",
     "start_time": "2025-02-04T01:46:34.798079200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from textwrap import dedent\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:46:42.865015200Z",
     "start_time": "2025-02-04T01:46:42.858013900Z"
    }
   },
   "id": "174a428754203b8d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.028495700Z",
     "start_time": "2025-02-04T01:46:42.861014400Z"
    }
   },
   "id": "6b10ae2259f0090a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PAD_TOKEN = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.029498Z",
     "start_time": "2025-02-04T01:46:59.818500100Z"
    }
   },
   "id": "98be22afca0f5525",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth's surface.\n",
    "\n",
    "    Parameters:\n",
    "    lat1, lon1: Latitude and Longitude of the first point in decimal degrees.\n",
    "    lat2, lon2: Latitude and Longitude of the second point in decimal degrees.\n",
    "\n",
    "    Returns:\n",
    "    Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Differences in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    # Distance\n",
    "    distance = R * c\n",
    "    return f\"{distance:.2f}km\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.030496500Z",
     "start_time": "2025-02-04T01:46:59.829498700Z"
    }
   },
   "id": "61e06f0a7b42dcdd",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_lat_long(entity):\n",
    "\n",
    "    words = entity.lower().split()\n",
    "    for i,word in enumerate(words):\n",
    "        if words[i-2] == 'latitude' and words[i-1] == 'val':\n",
    "          latitude = float(word)\n",
    "          longitude = float(words[i+4])\n",
    "    return  latitude, longitude"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.042495300Z",
     "start_time": "2025-02-04T01:46:59.837497Z"
    }
   },
   "id": "41af39b48cd92ad3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input file and extracts entity pairs and labels.\n",
    "    :param file_path: Path to the input text file.\n",
    "    :return: A list of tuples (entity_1, entity_2, label).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels=[]\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            ent_1_lat, ent_1_lon = get_lat_long(parts[0])\n",
    "            entity_1 = parts[0].replace(\"COL \", \"\").replace(\"[VAL] \", \"\").replace(\"VAL \", \"\").replace(\"name \", \"\").replace(\"type \", \"\").replace(\"latitude \", \"\").replace(\"longitude \", \"\").replace(\"postalCode \", \"\").replace(\"address \", \"\").strip()\n",
    "            ent_2_lat, ent_2_lon = get_lat_long(parts[1])\n",
    "            entity_2 = parts[1].replace(\"COL \", \"\").replace(\"[VAL] \", \"\").replace(\"VAL \", \"\").replace(\"name \", \"\").replace(\"type \", \"\").replace(\"latitude \", \"\").replace(\"longitude \", \"\").replace(\"postalCode \", \"\").replace(\"address \", \"\").strip()\n",
    "            label = parts[2]  \n",
    "            dist = haversine_distance(ent_1_lat, ent_1_lon, ent_2_lat, ent_2_lon)\n",
    "            data.append((entity_1, entity_2, dist))\n",
    "            labels.append(label)\n",
    "    return data, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.043495600Z",
     "start_time": "2025-02-04T01:46:59.841496500Z"
    }
   },
   "id": "5fbdc8308a5b6d4e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse_file_att_val(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input file and extracts entity pairs and labels.\n",
    "    :param file_path: Path to the input text file.\n",
    "    :return: A list of tuples (entity_1, entity_2, label).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels=[]\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            ent_1_lat, ent_1_lon = get_lat_long(parts[0])\n",
    "            entity_1 = parts[0].replace(\"COL \", \"\").replace(\"[VAL] \", \"\").replace(\"VAL \", \"\").replace(\"name \", \"name: \").replace(\"type \", \"type: \").replace(\"latitude \", \"latitude: \").replace(\"longitude \", \"longitude: \").replace(\"postalCode \", \"postalCode: \").replace(\"address \", \"address: \").strip()\n",
    "            ent_2_lat, ent_2_lon = get_lat_long(parts[1])\n",
    "            entity_2 = parts[1].replace(\"COL \", \"\").replace(\"[VAL] \", \"\").replace(\"VAL \", \"\").replace(\"name \", \"name: \").replace(\"type \", \"type: \").replace(\"latitude \", \"latitude: \").replace(\"longitude \", \"longitude: \").replace(\"postalCode \", \"postalCode: \").replace(\"address \", \"address: \").strip()\n",
    "            label = parts[2] \n",
    "            dist = haversine_distance(ent_1_lat, ent_1_lon, ent_2_lat, ent_2_lon)\n",
    "            data.append((entity_1, entity_2, dist))\n",
    "            labels.append(label)\n",
    "    return data, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.043495600Z",
     "start_time": "2025-02-04T01:46:59.852496400Z"
    }
   },
   "id": "85e1e030396919b8",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse_file_plm(file_path):\n",
    "    \"\"\"\n",
    "    Parses the input file and extracts entity pairs and labels.\n",
    "    :param file_path: Path to the input text file.\n",
    "    :return: A list of tuples (entity_1, entity_2, label).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels=[]\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            ent_1_lat, ent_1_lon = get_lat_long(parts[0])\n",
    "            # entity_1 = parts[0].replace(\"COL \", \"\").replace(\"[VAL] \", \"\").replace(\"VAL \", \"\").replace(\"name \", \"name: \").replace(\"type \", \"type: \").replace(\"latitude \", \"latitude: \").replace(\"longitude \", \"longitude: \").replace(\"postalCode \", \"postalCode: \").replace(\"address \", \"address: \").strip()\n",
    "            # ent_2_lat, ent_2_lon = get_lat_long(parts[1])\n",
    "            # entity_2 = parts[1].replace(\"COL \", \"\").replace(\"[VAL] \", \"\").replace(\"VAL \", \"\").replace(\"name \", \"name: \").replace(\"type \", \"type: \").replace(\"latitude \", \"latitude: \").replace(\"longitude \", \"longitude: \").replace(\"postalCode \", \"postalCode: \").replace(\"address \", \"address: \").strip()\n",
    "            label = parts[2] \n",
    "            # dist = haversine_distance(ent_1_lat, ent_1_lon, ent_2_lat, ent_2_lon)\n",
    "            data.append((parts[0], parts[1]))\n",
    "            labels.append(label)\n",
    "    return data, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.044500300Z",
     "start_time": "2025-02-04T01:46:59.867500700Z"
    }
   },
   "id": "5aa10901ebfbc51b",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_example(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place1: '{row[\"e1\"]}'\n",
    "    Place2: '{row[\"e2\"]}'\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.045498900Z",
     "start_time": "2025-02-04T01:46:59.874495500Z"
    }
   },
   "id": "1da3714636b76d4c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_example_distance(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place1: '{row[\"e1\"]}'\n",
    "    Place2: '{row[\"e2\"]}'\n",
    "    Distance: {row[\"distance\"]}\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions and the geographic distance between them is provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.046498100Z",
     "start_time": "2025-02-04T01:46:59.879495600Z"
    }
   },
   "id": "27f20eb31dd756b3",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_example_gtminer(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place 1: '{row[\"e1\"]}'\n",
    "    Place 2: '{row[\"e2\"]}'\n",
    "    Answer only with: same_as, part_of, serves, unknown\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.046498100Z",
     "start_time": "2025-02-04T01:46:59.888501500Z"
    }
   },
   "id": "3b02ba698a9b7f25",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_example_gtminer_distance(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place1: '{row[\"e1\"]}'\n",
    "    Place2: '{row[\"e2\"]}'\n",
    "    Distance: {row[\"distance\"]}\n",
    "    Answer only with: same_as, part_of, serves, unknown\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions and the geographic distance between them are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.047497700Z",
     "start_time": "2025-02-04T01:46:59.893500300Z"
    }
   },
   "id": "825e89fcc1e94064",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_example_gtminer_simple(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    Place1: '{row[\"e1\"]}'\n",
    "    Place2: '{row[\"e2\"]}'\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Two place descriptions are provided. Predict the relation between them. Answer only with ‘same_as’, ‘part_of’, ‘serves’ or ‘unknown’.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.073492500Z",
     "start_time": "2025-02-04T01:46:59.906495Z"
    }
   },
   "id": "668c1dda59dd5eac",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_paths = [\"data\\\\NZER\\\\auck\\\\\", \n",
    "                        \"data\\\\NZER\\\\hope\\\\\", \n",
    "                        \"data\\\\NZER\\\\norse\\\\\", \n",
    "                        \"data\\\\NZER\\\\north\\\\\", \n",
    "                        \"data\\\\NZER\\\\palm\\\\\", \n",
    "                        \"data\\\\GEOD_OSM_FSQ\\\\edi\\\\\", \n",
    "                        \"data\\\\GEOD_OSM_FSQ\\\\pit\\\\\", \n",
    "                        \"data\\\\GEOD_OSM_FSQ\\\\sin\\\\\",\n",
    "                        \"data\\\\GEOD_OSM_FSQ\\\\tor\\\\\",\n",
    "                        \"data\\\\GEOD_OSM_YELP\\\\edi\\\\\",\n",
    "                        \"data\\\\GEOD_OSM_YELP\\\\pit\\\\\",\n",
    "                        \"data\\\\GEOD_OSM_YELP\\\\sin\\\\\",\n",
    "                        \"data\\\\GEOD_OSM_YELP\\\\tor\\\\\",\n",
    "                        \"data\\\\SGN\\\\swiss\\\\\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.074495500Z",
     "start_time": "2025-02-04T01:46:59.910495600Z"
    }
   },
   "id": "8fb883140f5b4cb3",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompts = ['simple', 'attribute_val', 'plm', 'attribute_value_dist']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.075495100Z",
     "start_time": "2025-02-04T01:46:59.917494700Z"
    }
   },
   "id": "60a1c66ba35f0ecf",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\NZER\\\\auck\\\\train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompt \u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimple\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m prompt \u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattribute_val\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m prompt \u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplm\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m prompt\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimple\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m----> 6\u001B[0m         train_dataset, train_labels \u001B[38;5;241m=\u001B[39m \u001B[43mparse_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_folder_path\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m         valid_dataset, valid_labels \u001B[38;5;241m=\u001B[39m parse_file(dataset_folder_path\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid.txt\u001B[39m\u001B[38;5;124m'\u001B[39m) \n\u001B[0;32m      8\u001B[0m         test_dataset, test_labels \u001B[38;5;241m=\u001B[39m parse_file(dataset_folder_path\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest.txt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m, in \u001B[0;36mparse_file\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      8\u001B[0m labels\u001B[38;5;241m=\u001B[39m[]\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f\u001B[38;5;241m.\u001B[39mreadlines():\n\u001B[0;32m     11\u001B[0m         parts \u001B[38;5;241m=\u001B[39m line\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data\\\\NZER\\\\auck\\\\train.txt'"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    for dataset_folder_path in dataset_folder_paths:\n",
    "    \n",
    "        if prompt ==\"simple\" or prompt ==\"attribute_val\" or prompt ==\"plm\":\n",
    "            if prompt==\"simple\":\n",
    "                train_dataset, train_labels = parse_file(dataset_folder_path+'train.txt')\n",
    "                valid_dataset, valid_labels = parse_file(dataset_folder_path+'valid.txt') \n",
    "                test_dataset, test_labels = parse_file(dataset_folder_path+'test.txt')\n",
    "            elif prompt==\"attribute_val\":\n",
    "                train_dataset, train_labels = parse_file_att_val(dataset_folder_path+'train.txt')\n",
    "                valid_dataset, valid_labels = parse_file_att_val(dataset_folder_path+'valid.txt') \n",
    "                test_dataset, test_labels = parse_file_att_val(dataset_folder_path+'test.txt')\n",
    "            else:\n",
    "                train_dataset, train_labels = parse_file_plm(dataset_folder_path+'train.txt')\n",
    "                valid_dataset, valid_labels = parse_file_plm(dataset_folder_path+'valid.txt') \n",
    "                test_dataset, test_labels = parse_file_plm(dataset_folder_path+'test.txt')\n",
    "                \n",
    "            rows = []\n",
    "            for x,y in zip(train_dataset, train_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"answer\": [\"Yes\" if y==\"1\" else \"No\"][0],\n",
    "                    }\n",
    "                )\n",
    "            train_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(valid_dataset, valid_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"answer\": [\"Yes\" if y==\"1\" else \"No\"][0],\n",
    "                    }\n",
    "                )\n",
    "            valid_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(test_dataset, test_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"answer\": [\"Yes\" if y==\"1\" else \"No\"][0],\n",
    "                    }\n",
    "                )\n",
    "            test_df = pd.DataFrame(rows)\n",
    "            \n",
    "            train_df[\"text\"] = train_df.apply(format_example, axis=1)\n",
    "            valid_df[\"text\"] = valid_df.apply(format_example, axis=1)\n",
    "            test_df[\"text\"] = test_df.apply(format_example, axis=1)\n",
    "            \n",
    "        elif prompt ==\"attribute_value_dist\":\n",
    "            train_dataset, train_labels = parse_file_att_val(dataset_folder_path+'train.txt')\n",
    "            valid_dataset, valid_labels = parse_file_att_val(dataset_folder_path+'valid.txt') \n",
    "            test_dataset, test_labels = parse_file_att_val(dataset_folder_path+'test.txt')\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(train_dataset, train_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"distance\": x[2],\n",
    "                        \"answer\": [\"Yes\" if y==\"1\" else \"No\"][0],\n",
    "                    }\n",
    "                )\n",
    "            train_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(valid_dataset, valid_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"distance\": x[2],\n",
    "                        \"answer\": [\"Yes\" if y==\"1\" else \"No\"][0],\n",
    "                    }\n",
    "                )\n",
    "            valid_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(test_dataset, test_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"distance\": x[2],\n",
    "                        \"answer\": [\"Yes\" if y==\"1\" else \"No\"][0],\n",
    "                    }\n",
    "                )\n",
    "            test_df = pd.DataFrame(rows)\n",
    "\n",
    "            train_df[\"text\"] = train_df.apply(format_example_distance, axis=1)\n",
    "            valid_df[\"text\"] = valid_df.apply(format_example_distance, axis=1)\n",
    "            test_df[\"text\"] = test_df.apply(format_example_distance, axis=1)\n",
    "    \n",
    "    \n",
    "        dataset_output_path = dataset_folder_path.split('\\\\')[-3:-1]\n",
    "    \n",
    "        dataset_output_path_1, dataset_output_path_2 = dataset_output_path[0], dataset_output_path[1]\n",
    "    \n",
    "        train_out_file_path = \"datasets\\\\\"+ dataset_output_path_1 + \"_\"+ prompt + \"\\\\\"+ dataset_output_path_2+\"\\\\train.json\"\n",
    "        valid_out_file_path = \"datasets\\\\\"+ dataset_output_path_1 + \"_\"+ prompt + \"\\\\\"+ dataset_output_path_2+\"\\\\valid.json\"\n",
    "        test_out_file_path = \"datasets\\\\\"+ dataset_output_path_1 + \"_\"+ prompt + \"\\\\\"+ dataset_output_path_2+\"\\\\test.json\"\n",
    "    \n",
    "        os.makedirs(os.path.dirname(train_out_file_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(valid_out_file_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(test_out_file_path), exist_ok=True)\n",
    "    \n",
    "        train_df.to_json(train_out_file_path, orient=\"records\", lines=True)\n",
    "        valid_df.to_json(valid_out_file_path, orient=\"records\", lines=True)\n",
    "        test_df.to_json(test_out_file_path, orient=\"records\", lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.654491400Z",
     "start_time": "2025-02-04T01:46:59.923494900Z"
    }
   },
   "id": "aa16f43689c3d908",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path_gt = ['data\\\\GTMD\\\\mel\\\\', \n",
    "                'data\\\\GTMD\\\\sea\\\\',\n",
    "                'data\\\\GTMD\\\\sin\\\\',\n",
    "                'data\\\\GTMD\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-04T01:47:00.653491200Z"
    }
   },
   "id": "751689bed72643ee",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for prompt in prompts:    \n",
    "    for dataset_folder_path in file_path_gt:\n",
    "        train_dataset, train_labels = parse_file_plm(dataset_folder_path+'train.txt')\n",
    "        test_dataset, test_labels = parse_file_plm(dataset_folder_path+'test.txt')\n",
    "        valid_dataset, valid_labels = parse_file_plm(dataset_folder_path+'valid.txt') \n",
    "        \n",
    "        \n",
    "        if prompt ==\"simple\" or prompt ==\"attribute_val\" or prompt ==\"plm\":\n",
    "            if prompt==\"simple\":\n",
    "                train_dataset, train_labels = parse_file(dataset_folder_path+'train.txt')\n",
    "                valid_dataset, valid_labels = parse_file(dataset_folder_path+'valid.txt') \n",
    "                test_dataset, test_labels = parse_file(dataset_folder_path+'test.txt')\n",
    "            elif prompt==\"attribute_val\":\n",
    "                train_dataset, train_labels = parse_file_att_val(dataset_folder_path+'train.txt')\n",
    "                valid_dataset, valid_labels = parse_file_att_val(dataset_folder_path+'valid.txt') \n",
    "                test_dataset, test_labels = parse_file_att_val(dataset_folder_path+'test.txt')\n",
    "            else:\n",
    "                train_dataset, train_labels = parse_file_plm(dataset_folder_path+'train.txt')\n",
    "                valid_dataset, valid_labels = parse_file_plm(dataset_folder_path+'valid.txt') \n",
    "                test_dataset, test_labels = parse_file_plm(dataset_folder_path+'test.txt')\n",
    "                \n",
    "            rows = []\n",
    "            for x,y in zip(train_dataset, train_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        # \"distance\": x[2],\n",
    "                        \"answer\": [\"same_as\" if y==\"1\" else \"part_of\" if y==\"2\" else \"serves\" if y==\"3\" else \"unknown\" if y==\"0\" else \"asd\"][0],\n",
    "                    }\n",
    "                )\n",
    "            train_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(valid_dataset, valid_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        # \"distance\": x[2],\n",
    "                        \"answer\": [\"same_as\" if y==\"1\" else \"part_of\" if y==\"2\" else \"serves\" if y==\"3\" else \"unknown\" if y==\"0\" else \"asd\"][0],\n",
    "                    }\n",
    "                )\n",
    "            valid_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(test_dataset, test_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        # \"distance\": x[2],\n",
    "                        \"answer\":  [\"same_as\" if y==\"1\" else \"part_of\" if y==\"2\" else \"serves\" if y==\"3\" else \"unknown\" if y==\"0\" else \"asd\"][0],\n",
    "                    }\n",
    "                )\n",
    "            test_df = pd.DataFrame(rows)\n",
    "            \n",
    "            if prompt==\"simple\":\n",
    "                train_df[\"text\"] = train_df.apply(format_example_gtminer_simple, axis=1)\n",
    "                valid_df[\"text\"] = valid_df.apply(format_example_gtminer_simple, axis=1)\n",
    "                test_df[\"text\"] = test_df.apply(format_example_gtminer_simple, axis=1)\n",
    "            else:\n",
    "                train_df[\"text\"] = train_df.apply(format_example_gtminer, axis=1)\n",
    "                valid_df[\"text\"] = valid_df.apply(format_example_gtminer, axis=1)\n",
    "                test_df[\"text\"] = test_df.apply(format_example_gtminer, axis=1)\n",
    "            \n",
    "            \n",
    "            \n",
    "        elif prompt =='attribute_value_dist':\n",
    "            \n",
    "            train_dataset, train_labels = parse_file_att_val(dataset_folder_path+'train.txt')\n",
    "            valid_dataset, valid_labels = parse_file_att_val(dataset_folder_path+'valid.txt') \n",
    "            test_dataset, test_labels = parse_file_att_val(dataset_folder_path+'test.txt')\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(train_dataset, train_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"distance\": x[2],\n",
    "                        \"answer\": [\"same_as\" if y==\"1\" else \"part_of\" if y==\"2\" else \"serves\" if y==\"3\" else \"unknown\" if y==\"0\" else \"asd\"][0],\n",
    "                    }\n",
    "                )\n",
    "            train_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(valid_dataset, valid_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"distance\": x[2],\n",
    "                        \"answer\": [\"same_as\" if y==\"1\" else \"part_of\" if y==\"2\" else \"serves\" if y==\"3\" else \"unknown\" if y==\"0\" else \"asd\"][0],\n",
    "                    }\n",
    "                )\n",
    "            valid_df = pd.DataFrame(rows)\n",
    "            \n",
    "            rows = []\n",
    "            for x,y in zip(test_dataset, test_labels):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"e1\": x[0],\n",
    "                        \"e2\": x[1],\n",
    "                        \"distance\": x[2],\n",
    "                        \"answer\":  [\"same_as\" if y==\"1\" else \"part_of\" if y==\"2\" else \"serves\" if y==\"3\" else \"unknown\" if y==\"0\" else \"asd\"][0],\n",
    "                    }\n",
    "                )\n",
    "            test_df = pd.DataFrame(rows)\n",
    "            \n",
    "            train_df[\"text\"] = train_df.apply(format_example_gtminer_distance, axis=1)\n",
    "            valid_df[\"text\"] = valid_df.apply(format_example_gtminer_distance, axis=1)\n",
    "            test_df[\"text\"] = test_df.apply(format_example_gtminer_distance, axis=1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        dataset_output_path = dataset_folder_path.split('\\\\')[-3:-1]\n",
    "        \n",
    "        dataset_output_path_1, dataset_output_path_2 = dataset_output_path[0], dataset_output_path[1]\n",
    "        \n",
    "        train_out_file_path = \"datasets\\\\\"+ dataset_output_path_1 +\"_\"+ prompt+ \"\\\\\"+ dataset_output_path_2+\"\\\\train.json\"\n",
    "        valid_out_file_path = \"datasets\\\\\"+ dataset_output_path_1 +\"_\"+ prompt+ \"\\\\\"+ dataset_output_path_2+\"\\\\valid.json\"\n",
    "        test_out_file_path = \"datasets\\\\\"+ dataset_output_path_1 +\"_\"+ prompt+ \"\\\\\"+ dataset_output_path_2+\"\\\\test.json\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(train_out_file_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(valid_out_file_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(test_out_file_path), exist_ok=True)\n",
    "        \n",
    "        train_df.to_json(train_out_file_path, orient=\"records\", lines=True)\n",
    "        valid_df.to_json(valid_out_file_path, orient=\"records\", lines=True)\n",
    "        test_df.to_json(test_out_file_path, orient=\"records\", lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T01:47:00.658493200Z",
     "start_time": "2025-02-04T01:47:00.654491400Z"
    }
   },
   "id": "d463750a0c5d19a7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
