{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:15:12.773200Z",
     "start_time": "2025-02-04T02:14:29.288650700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from metrics import calc_mets_my, calculate_metrics2\n",
    "from datasets import Dataset, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROJECT = \"Llama3-8B-QLora-Omni\"\n",
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:15:12.779205800Z",
     "start_time": "2025-02-04T02:15:12.774202400Z"
    }
   },
   "id": "717a14b47b432706",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda:0'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:15:14.304193500Z",
     "start_time": "2025-02-04T02:15:12.781202600Z"
    }
   },
   "id": "84893e79016722b0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:15:14.863190100Z",
     "start_time": "2025-02-04T02:15:14.305195400Z"
    }
   },
   "id": "4a0dd64fdc5cbfbe",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6e8f00b265a4c5482407bc04e9944df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=True\n",
    ")\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:23.661650300Z",
     "start_time": "2025-02-04T02:15:14.867193Z"
    }
   },
   "id": "1145fa5b1ec3e832",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")\n",
    "PAD_TOKEN = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.724637500Z",
     "start_time": "2025-02-04T02:16:23.656649400Z"
    }
   },
   "id": "b5eea3353549e3e9",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.207535028457642 GB\n"
     ]
    }
   ],
   "source": [
    "memory_used = model.get_memory_footprint()\n",
    "print(\"Memory footprint: {} GB\".format(memory_used/1024/1024/1024))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.748634600Z",
     "start_time": "2025-02-04T02:16:24.725638100Z"
    }
   },
   "id": "e406f14be9fe68e7",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_simple(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2 = row['e1'], row['e2']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Answer: \"\"\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.768634800Z",
     "start_time": "2025-02-04T02:16:24.743636400Z"
    }
   },
   "id": "2c1a5fad3e3a9cb0",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_attribute_value_distance(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2, distance = row['e1'], row['e2'], row['distance']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions and the geographic distance between them is provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Distance: {distance}\n",
    "    Answer: \"\"\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.770636800Z",
     "start_time": "2025-02-04T02:16:24.765636900Z"
    }
   },
   "id": "c40163da93bbb941",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.781636500Z",
     "start_time": "2025-02-04T02:16:24.771635500Z"
    }
   },
   "id": "4d439e4e53b585fb",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def zero_shot_inference(model, tokenizer, prompts, max_new_tokens):\n",
    "    \"\"\"\n",
    "    Performs zero-shot inference using the model.\n",
    "    :param model: The loaded quantized model.\n",
    "    :param tokenizer: Tokenizer for the model.\n",
    "    :param prompts: List of input prompts.\n",
    "    :return: Model predictions (Yes/No).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        # outputs = model.pipeline(inputs.input_ids)\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_length=100,  # Maximum length of the generated text\n",
    "            max_new_tokens= max_new_tokens,\n",
    "            num_return_sequences=1,  # Number of sequences to generate\n",
    "            no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "            temperature=0.01,  # Controls randomness; lower is less random\n",
    "            top_k=50,  # Top-k sampling\n",
    "        )\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "        results.append(prediction.strip())\n",
    "        \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.823636Z",
     "start_time": "2025-02-04T02:16:24.790637Z"
    }
   },
   "id": "8cb921151e0059dd",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    " \n",
    "    # Convert \"Yes\" to 1 and \"No\" to 0 for predicted labels\n",
    "    predicted = [1 if label == \"Yes\" else 0 if label == \"No\" else 3 for label in predictions]\n",
    "    \n",
    "    # Ensure ground truth is already in binary format\n",
    "    ground_truth = [int(x) for x in labels]\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(ground_truth, predicted)\n",
    "    recall = recall_score(ground_truth, predicted)\n",
    "    f1 = f1_score(ground_truth, predicted)\n",
    "    \n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.825634800Z",
     "start_time": "2025-02-04T02:16:24.795636900Z"
    }
   },
   "id": "b7723a5d16c9abcb",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot. Select between \"simple\", \"attribute_val\", \"plm\" and \"attribute_value_dist\"\n",
    "test_prompt = \"plm\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.825634800Z",
     "start_time": "2025-02-04T02:16:24.814640600Z"
    }
   },
   "id": "7e39dbf42f66ceaf",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\NZER_'+ test_prompt+ '\\\\auck\\\\', \n",
    "                       'datasets\\\\NZER_'+ test_prompt+ '\\\\hope\\\\', \n",
    "                       'datasets\\\\NZER_'+ test_prompt+ '\\\\norse\\\\',\n",
    "                       'datasets\\\\NZER_'+ test_prompt+ '\\\\north\\\\', \n",
    "                       'datasets\\\\NZER_'+ test_prompt+ '\\\\palm\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\SGN_'+test_prompt+'\\\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.842640800Z",
     "start_time": "2025-02-04T02:16:24.831637900Z"
    }
   },
   "id": "c421837cbbcff5f7",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# File path to the input data\n",
    "for dataset_folder in dataset_folder_path:\n",
    "    \n",
    "    # print(dataset_folder.split('\\\\')[-3:])\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    \n",
    "    # data, labels = parse_file(dataset_folder)\n",
    "    labels = [x['answer'] for x in dataset[\"test\"]]\n",
    "    if test_prompt==\"attribute_value_dist\":\n",
    "        prompts = [prepare_prompt_attribute_value_distance(row) for row in dataset['test']]\n",
    "    else:\n",
    "        prompts = [prepare_prompt_simple(row) for row in dataset['test']]\n",
    "    print(prompts[0])\n",
    "    print(labels[0])\n",
    "    predictions = zero_shot_inference(model, tokenizer, prompts, 1)\n",
    "    predictions = [x.split(\" \")[-1].strip() for x in predictions] \n",
    "    # predictions = [1 if label in [\"Yes\", \"yes\"] else 2 if label in [\"no\", \"No\"] else 3 for label in predictions]\n",
    "    print(len(predictions), len(labels))\n",
    "    print(calculate_metrics(predictions, [1 if lbl ==\"Yes\" else 0 if lbl ==\"No\" else 3 for lbl in labels]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fc57c609c25ec84",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_gtminer_simple(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2 = row['e1'], row['e2']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions are provided. Predict the relation between them. Answer only with ‘same_as’, ‘part_of’, ‘serves’ or ‘unknown’..\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.857641200Z",
     "start_time": "2025-02-04T02:16:24.836644200Z"
    }
   },
   "id": "de1254ad98a8991f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_gtminer(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2 = row['e1'], row['e2']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.858636300Z",
     "start_time": "2025-02-04T02:16:24.849636900Z"
    }
   },
   "id": "3d8340c4b05fe6e3",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_gtminer_distance(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2, dist = row['e1'], row['e2'], row['distance']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions and the geographic distance between them are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations..\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Distance: {dist}\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.874632900Z",
     "start_time": "2025-02-04T02:16:24.855635100Z"
    }
   },
   "id": "4fada91b9f63e897",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot for Geospatial relation mining task. Select between \"simple\", \"attribute_val\", \"plm\" and \"attribute_value_dist\"\n",
    "test_prompt = \"plm\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.875634700Z",
     "start_time": "2025-02-04T02:16:24.862635500Z"
    }
   },
   "id": "f35603dfda1c5b1b",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets\\\\GTMD_'+ test_prompt+ '\\\\mel\\\\', \n",
    "                       'datasets\\\\GTMD_'+ test_prompt+ '\\\\sea\\\\', \n",
    "                       'datasets\\\\GTMD_'+ test_prompt+ '\\\\sin\\\\',\n",
    "                       'datasets\\\\GTMD_'+ test_prompt+ '\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T02:16:24.887636800Z",
     "start_time": "2025-02-04T02:16:24.869633Z"
    }
   },
   "id": "bf9122b8a7fd0a6",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    \n",
    "    # data, labels = parse_file(dataset_folder)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    if test_prompt==\"attribute_value_dist\":\n",
    "        prompts = [prepare_prompt_gtminer_distance(row) for row in dataset['test']]\n",
    "    elif test_prompt==\"simple\":\n",
    "        prompts = [prepare_prompt_gtminer_simple(row) for row in dataset['test']]\n",
    "    else:\n",
    "        prompts = [prepare_prompt_gtminer(row) for row in dataset['test']]\n",
    "    print(prompts[0])\n",
    "    print(dataset['test'][0]['answer'])\n",
    "    predictions = zero_shot_inference(model, tokenizer, prompts, 2)\n",
    "    predictions = [x.split(\": \")[-1].strip() for x in predictions] \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    print(len(predictions), len(labels))\n",
    "    print(calculate_metrics2(predictions, labels))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66e4bbbfd4ed9de7",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
