{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:46:14.563247300Z",
     "start_time": "2025-02-03T04:46:14.548247700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, logging\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from metrics import calc_mets_my, calculate_metrics2\n",
    "from datasets import Dataset, load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROJECT = \"Llama3-8B-QLora-Omni\"\n",
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:35:56.695044900Z",
     "start_time": "2025-02-03T04:35:56.683046300Z"
    }
   },
   "id": "717a14b47b432706",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda:0'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:36:02.316585700Z",
     "start_time": "2025-02-03T04:35:56.689047600Z"
    }
   },
   "id": "84893e79016722b0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:36:02.793587900Z",
     "start_time": "2025-02-03T04:36:02.312583Z"
    }
   },
   "id": "4a0dd64fdc5cbfbe",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7b9e6bc3d5a41ea93e0d84bd95845bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=True\n",
    ")\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:11.701146300Z",
     "start_time": "2025-02-03T04:36:02.795581500Z"
    }
   },
   "id": "1145fa5b1ec3e832",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=True\n",
    ")\n",
    "PAD_TOKEN = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN})\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:12.956138900Z",
     "start_time": "2025-02-03T04:37:11.695145700Z"
    }
   },
   "id": "b5eea3353549e3e9",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5.207535028457642 GB\n"
     ]
    }
   ],
   "source": [
    "memory_used = model.get_memory_footprint()\n",
    "print(\"Memory footprint: {} GB\".format(memory_used/1024/1024/1024))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:12.978140500Z",
     "start_time": "2025-02-03T04:37:12.965140500Z"
    }
   },
   "id": "e406f14be9fe68e7",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_simple(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2 = row['e1'], row['e2']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Answer: \"\"\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:12.999137Z",
     "start_time": "2025-02-03T04:37:12.976138400Z"
    }
   },
   "id": "2c1a5fad3e3a9cb0",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_attribute_value_distance(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2, distance = row['e1'], row['e2'], row['distance']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions and the geographic distance between them is provided. Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Distance: {distance}\n",
    "    Answer: \"\"\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:13.000140200Z",
     "start_time": "2025-02-03T04:37:12.983142Z"
    }
   },
   "id": "c40163da93bbb941",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:13.035148500Z",
     "start_time": "2025-02-03T04:37:13.003137900Z"
    }
   },
   "id": "4d439e4e53b585fb",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def zero_shot_inference(model, tokenizer, prompts, max_new_tokens):\n",
    "    \"\"\"\n",
    "    Performs zero-shot inference using the model.\n",
    "    :param model: The loaded quantized model.\n",
    "    :param tokenizer: Tokenizer for the model.\n",
    "    :param prompts: List of input prompts.\n",
    "    :return: Model predictions (Yes/No).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        # outputs = model.pipeline(inputs.input_ids)\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids, \n",
    "            max_length=100,  # Maximum length of the generated text\n",
    "            max_new_tokens= max_new_tokens,\n",
    "            num_return_sequences=1,  # Number of sequences to generate\n",
    "            no_repeat_ngram_size=2,  # Avoid repeating phrases\n",
    "            temperature=0.01,  # Controls randomness; lower is less random\n",
    "            top_k=50,  # Top-k sampling\n",
    "        )\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\n",
    "        results.append(prediction.strip())\n",
    "        \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:37:13.037138Z",
     "start_time": "2025-02-03T04:37:13.013146100Z"
    }
   },
   "id": "8cb921151e0059dd",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    " \n",
    "    # Convert \"Yes\" to 1 and \"No\" to 0 for predicted labels\n",
    "    predicted = [1 if label == \"Yes\" else 0 if label == \"No\" else 3 for label in predictions]\n",
    "    \n",
    "    # Ensure ground truth is already in binary format\n",
    "    ground_truth = [int(x) for x in labels]\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(ground_truth, predicted)\n",
    "    recall = recall_score(ground_truth, predicted)\n",
    "    f1 = f1_score(ground_truth, predicted)\n",
    "    \n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:47:40.316828900Z",
     "start_time": "2025-02-03T04:47:40.208828200Z"
    }
   },
   "id": "b7723a5d16c9abcb",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot. Select between \"simple\", \"attribute_value\", \"plm\" and \"attribute_value_dist\"\n",
    "test_prompt = \"plm\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:47:40.550827900Z",
     "start_time": "2025-02-03T04:47:40.532826400Z"
    }
   },
   "id": "7e39dbf42f66ceaf",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets2\\\\NZER_'+ test_prompt+ '\\\\auck\\\\', \n",
    "                       'datasets2\\\\NZER_'+ test_prompt+ '\\\\hope\\\\', \n",
    "                       'datasets2\\\\NZER_'+ test_prompt+ '\\\\norse\\\\',\n",
    "                       'datasets2\\\\NZER_'+ test_prompt+ '\\\\north\\\\', \n",
    "                       'datasets2\\\\NZER_'+ test_prompt+ '\\\\palm\\\\', \n",
    "                       'datasets2\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_FSQ_'+ test_prompt+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\edi\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\pit\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\sin\\\\', \n",
    "                       'datasets\\\\GEOD_OSM_YELP_'+ test_prompt+ '\\\\tor\\\\', \n",
    "                       'datasets\\\\SGN_'+test_prompt+'\\\\swiss\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:47:41.381822900Z",
     "start_time": "2025-02-03T04:47:41.358824400Z"
    }
   },
   "id": "c421837cbbcff5f7",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets2', 'NZER_plm', 'auck', '']\n",
      "Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
      "    Place 1: COL name VAL Tautini COL type VAL farmstead COL latitude VAL -40.18825 COL longitude VAL 176.14021 \n",
      "    Place 2: COL name VAL Waikoukou Stream COL type VAL Stream COL latitude VAL -40.08742665596005 COL longitude VAL 176.28878276483226 \n",
      "    Answer: \n",
      "No\n",
      "601 601\n",
      "{'Precision': 0.7142857142857143, 'Recall': 0.25, 'F1 Score': 0.37037037037037035}\n",
      "['datasets2', 'NZER_plm', 'hope', '']\n",
      "Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
      "    Place 1: COL name VAL Silver Stream COL type VAL stream COL latitude VAL -44.04833 COL longitude VAL 168.67008 \n",
      "    Place 2: COL name VAL Deep Dale COL type VAL Valley COL latitude VAL -44.025083 COL longitude VAL 168.672056 \n",
      "    Answer: \n",
      "No\n",
      "2907 2907\n",
      "{'Precision': 0.9, 'Recall': 0.08035714285714286, 'F1 Score': 0.14754098360655737}\n",
      "['datasets2', 'NZER_plm', 'norse', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77949d5348f842919629ba255bb2054a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f33e353f96f54abb82e8fa09bd6bc222"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d59d5856cb4b76bc79c21b9aebd2aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
      "    Place 1: COL name VAL Mangatewai River Scenic Reserve COL type VAL Scenic Reserve COL latitude VAL -39.985556 COL longitude VAL 176.254722 \n",
      "    Place 2: COL name VAL Ōtāwhao COL type VAL locality COL latitude VAL -40.0501529 COL longitude VAL 176.2682748 \n",
      "    Answer: \n",
      "No\n",
      "1783 1783\n",
      "{'Precision': 1.0, 'Recall': 0.06666666666666667, 'F1 Score': 0.125}\n",
      "['datasets2', 'NZER_plm', 'north', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cfdc07a18ed4e4eb18146b4f3a1434a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65a8854e4eef4994a06b9e09d5979703"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a08720f57f0d405392796931a9816a81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the two place descriptions refer to the same real-world place? Answer with 'Yes' if they do and 'No' if they do not.\n",
      "    Place 1: COL name VAL Waitiki Channel COL type VAL stream COL latitude VAL -34.51818 COL longitude VAL 172.88018 \n",
      "    Place 2: COL name VAL Matapia COL type VAL Island COL latitude VAL -34.606333571762384 COL longitude VAL 172.79844153380557 \n",
      "    Answer: \n",
      "No\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(prompts[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(labels[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m---> 22\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mzero_shot_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m predictions \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mstrip() \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m predictions] \n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# predictions = [1 if label in [\"Yes\", \"yes\"] else 2 if label in [\"no\", \"No\"] else 3 for label in predictions]\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[11], line 14\u001B[0m, in \u001B[0;36mzero_shot_inference\u001B[1;34m(model, tokenizer, prompts, max_new_tokens)\u001B[0m\n\u001B[0;32m     12\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer(prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# outputs = model.pipeline(inputs.input_ids)\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Maximum length of the generated text\u001B[39;49;00m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Number of sequences to generate\u001B[39;49;00m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mno_repeat_ngram_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Avoid repeating phrases\u001B[39;49;00m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Controls randomness; lower is less random\u001B[39;49;00m\n\u001B[0;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Top-k sampling\u001B[39;49;00m\n\u001B[0;32m     22\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m prediction \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# prediction = tokenizer.decode(outputs[:, inputs.shape[1]:])\u001B[39;00m\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2252\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   2244\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2245\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2246\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2247\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2248\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2249\u001B[0m     )\n\u001B[0;32m   2251\u001B[0m     \u001B[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2252\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sample(\n\u001B[0;32m   2253\u001B[0m         input_ids,\n\u001B[0;32m   2254\u001B[0m         logits_processor\u001B[38;5;241m=\u001B[39mprepared_logits_processor,\n\u001B[0;32m   2255\u001B[0m         stopping_criteria\u001B[38;5;241m=\u001B[39mprepared_stopping_criteria,\n\u001B[0;32m   2256\u001B[0m         generation_config\u001B[38;5;241m=\u001B[39mgeneration_config,\n\u001B[0;32m   2257\u001B[0m         synced_gpus\u001B[38;5;241m=\u001B[39msynced_gpus,\n\u001B[0;32m   2258\u001B[0m         streamer\u001B[38;5;241m=\u001B[39mstreamer,\n\u001B[0;32m   2259\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2260\u001B[0m     )\n\u001B[0;32m   2262\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2263\u001B[0m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[0;32m   2264\u001B[0m     beam_scorer \u001B[38;5;241m=\u001B[39m BeamSearchScorer(\n\u001B[0;32m   2265\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   2266\u001B[0m         num_beams\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_beams,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2271\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length,\n\u001B[0;32m   2272\u001B[0m     )\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:3251\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[0;32m   3248\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[0;32m   3250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[1;32m-> 3251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   3252\u001B[0m     is_prefill \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   3253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1163\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001B[0m\n\u001B[0;32m   1160\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   1162\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[1;32m-> 1163\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\n\u001B[0;32m   1164\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1165\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   1166\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   1167\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[0;32m   1168\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[0;32m   1169\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[0;32m   1170\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   1171\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   1172\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1173\u001B[0m     cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[0;32m   1174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1175\u001B[0m )\n\u001B[0;32m   1177\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1178\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:913\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[0;32m    901\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    902\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    903\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    910\u001B[0m         position_embeddings,\n\u001B[0;32m    911\u001B[0m     )\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 913\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m decoder_layer(\n\u001B[0;32m    914\u001B[0m         hidden_states,\n\u001B[0;32m    915\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mcausal_mask,\n\u001B[0;32m    916\u001B[0m         position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m    917\u001B[0m         past_key_value\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[0;32m    918\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m    919\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[0;32m    920\u001B[0m         cache_position\u001B[38;5;241m=\u001B[39mcache_position,\n\u001B[0;32m    921\u001B[0m         position_embeddings\u001B[38;5;241m=\u001B[39mposition_embeddings,\n\u001B[0;32m    922\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mflash_attn_kwargs,\n\u001B[0;32m    923\u001B[0m     )\n\u001B[0;32m    925\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    927\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:656\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[0;32m    654\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    655\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[1;32m--> 656\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    657\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[0;32m    659\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:242\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 242\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdown_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\omniLLM\\.venv\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:484\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    480\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n\u001B[0;32m    482\u001B[0m bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_dtype)\n\u001B[1;32m--> 484\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbnb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul_4bit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquant_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquant_state\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp_dtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# File path to the input data\n",
    "for dataset_folder in dataset_folder_path:\n",
    "    \n",
    "    # print(dataset_folder.split('\\\\')[-3:])\n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    \n",
    "    # data, labels = parse_file(dataset_folder)\n",
    "    labels = [x['answer'] for x in dataset[\"test\"]]\n",
    "    if test_prompt==\"attribute_value_dist\":\n",
    "        prompts = [prepare_prompt_attribute_value_distance(row) for row in dataset['test']]\n",
    "    else:\n",
    "        prompts = [prepare_prompt_simple(row) for row in dataset['test']]\n",
    "    print(prompts[0])\n",
    "    print(labels[0])\n",
    "    predictions = zero_shot_inference(model, tokenizer, prompts, 1)\n",
    "    predictions = [x.split(\" \")[-1].strip() for x in predictions] \n",
    "    # predictions = [1 if label in [\"Yes\", \"yes\"] else 2 if label in [\"no\", \"No\"] else 3 for label in predictions]\n",
    "    print(len(predictions), len(labels))\n",
    "    print(calculate_metrics(predictions, [1 if lbl ==\"Yes\" else 0 if lbl ==\"No\" else 3 for lbl in labels]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:12:36.524304200Z",
     "start_time": "2025-02-03T04:59:16.844410400Z"
    }
   },
   "id": "6fc57c609c25ec84",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_gtminer_simple(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2 = row['e1'], row['e2']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions are provided. Predict the relation between them. Answer only with ‘same_as’, ‘part_of’, ‘serves’ or ‘unknown’..\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:12:53.132940600Z",
     "start_time": "2025-02-03T05:12:53.128932700Z"
    }
   },
   "id": "de1254ad98a8991f",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_gtminer(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2 = row['e1'], row['e2']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:12:53.408932400Z",
     "start_time": "2025-02-03T05:12:53.402930100Z"
    }
   },
   "id": "3d8340c4b05fe6e3",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_prompt_gtminer_distance(row):\n",
    "    \"\"\"\n",
    "    Prepares a natural language prompt for the entity resolution task.\n",
    "    :param row: A tuple with two entities and the expected result.\n",
    "    :return: A formatted prompt string.\n",
    "    \"\"\"\n",
    "    \n",
    "    entity_1, entity_2, dist = row['e1'], row['e2'], row['distance']\n",
    "    # print(entity_1)\n",
    "    prompt = f\"\"\"Two place descriptions and the geographic distance between them are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations..\n",
    "    Place 1: {entity_1}\n",
    "    Place 2: {entity_2}\n",
    "    Distance: {dist}\n",
    "    Answer: \"\"\"\n",
    "    \n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:12:53.549933600Z",
     "start_time": "2025-02-03T05:12:53.543929300Z"
    }
   },
   "id": "4fada91b9f63e897",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select prompt to test zero shot for Geospatial relation mining task. Select between \"simple\", \"attribute_value\", \"plm\" and \"attribute_value_dist\"\n",
    "test_prompt = \"plm\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:12:53.769928700Z",
     "start_time": "2025-02-03T05:12:53.764027900Z"
    }
   },
   "id": "f35603dfda1c5b1b",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_folder_path = ['datasets2\\\\GTMD_'+ test_prompt+ '\\\\mel\\\\', \n",
    "                       'datasets2\\\\GTMD_'+ test_prompt+ '\\\\sea\\\\', \n",
    "                       'datasets2\\\\GTMD_'+ test_prompt+ '\\\\sin\\\\',\n",
    "                       'datasets2\\\\GTMD_'+ test_prompt+ '\\\\tor\\\\']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:12:54.432463Z",
     "start_time": "2025-02-03T05:12:54.420464200Z"
    }
   },
   "id": "bf9122b8a7fd0a6",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets2', 'GTMD_plm', 'mel', '']\n",
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\n",
      "    Place 1: COL name VAL JB Hi-Fi COL type VAL electronics COL address VAL nan COL latitude VAL -37.7681204 COL longitude VAL 145.304855 \n",
      "    Place 2: COL name VAL Chirnside Homemaker Centre COL type VAL mall COL address VAL 282 Maroondah Highway 3116 COL latitude VAL -37.7663845 COL longitude VAL 145.3058855 \n",
      "    Answer: \n",
      "part_of\n",
      "1839 1839\n",
      "P: 0.4786  |  R: 0.6586  |  F1: 0.5544\n",
      "{'precision': 0.4786269430051813, 'recall': 0.6586452762923352, 'f1': 0.5543885971492873}\n",
      "['datasets2', 'GTMD_plm', 'sea', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c605d5c5584647c3a92272100189977e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d27cd3fa05c5482a9dbc325529ce307f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d493a39a1e0643aa83639d0750c3a78f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\n",
      "    Place 1: COL name VAL 32 Bar & Grill COL type VAL Sports Bars; American (Traditional) COL address VAL nan COL latitude VAL 47.70645953432845 COL longitude VAL -122.3245641011371 \n",
      "    Place 2: COL name VAL Kraken Community Iceplex COL type VAL leisure COL address VAL 10601 5th Avenue Northeast 98125 COL latitude VAL 47.7062215 COL longitude VAL -122.3251917 \n",
      "    Answer: \n",
      "part_of\n",
      "4747 4747\n",
      "P: 0.3026  |  R: 0.6159  |  F1: 0.4058\n",
      "{'precision': 0.3025885900980146, 'recall': 0.6158567774936061, 'f1': 0.4057971014492754}\n",
      "['datasets2', 'GTMD_plm', 'sin', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "baa09f0ed8394ab0a24afd3cf3862ae1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcc27848a2c24e3fb18fef89bd00447d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b66e7d65c2aa436d81f618bc4aa680f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\n",
      "    Place 1: COL name VAL Garrett Gourmet Popcorn COL type VAL Candy Stores COL address VAL 541 Orchard Rd #01-K1 Liat Towers 238881 COL latitude VAL 1.3053032 COL longitude VAL 103.8305236 \n",
      "    Place 2: COL name VAL Liat Towers COL type VAL Shopping Centers COL address VAL 541 Orch Rd 238881 COL latitude VAL 1.3051056 COL longitude VAL 103.8307274 \n",
      "    Answer: \n",
      "part_of\n",
      "7852 7852\n",
      "P: 0.3356  |  R: 0.5645  |  F1: 0.421\n",
      "{'precision': 0.33562071116656267, 'recall': 0.5645330535152151, 'f1': 0.42097026604068855}\n",
      "['datasets2', 'GTMD_plm', 'tor', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3caad912fcc44fb9a7c8187ce965ca65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating valid split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff77e5dd3f4d43feb750fe6d3c74905d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7f0baa0d52345aba5c7b91c787e2671"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two place descriptions are provided. Answer with 'same_as' if the first place is the same as the second place. Answer with 'part_of' if the first place is a part of the second place and is located inside the second place. Answer with 'serves' if the first place provides a service to the second place in terms of human mobility, assistance, etc. Answer with 'unknown' if the two places show none of these relations.\n",
      "    Place 1: COL name VAL New College Library COL type VAL library COL address VAL 20 Willcocks Street COL latitude VAL 43.6617897 COL longitude VAL -79.4001637 \n",
      "    Place 2: COL name VAL Engineering Library COL type VAL library COL address VAL nan COL latitude VAL 43.6601686 COL longitude VAL -79.3949804 \n",
      "    Answer: \n",
      "unknown\n",
      "5101 5101\n",
      "P: 0.3712  |  R: 0.6408  |  F1: 0.4701\n",
      "{'precision': 0.37122687439143137, 'recall': 0.6407563025210085, 'f1': 0.4700986436498151}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for dataset_folder in dataset_folder_path:\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(dataset_folder.split(\"\\\\\"))\n",
    "    dataset_output_path_1, dataset_output_path_2 = dataset_folder.split(\"\\\\\")[-3], dataset_folder.split(\"\\\\\")[-2]\n",
    "        \n",
    "    dataset = load_dataset(\n",
    "        \"json\",\n",
    "        data_files={\"train\": dataset_folder+\"train.json\", \"valid\": dataset_folder+\"valid.json\", \"test\": dataset_folder+\"test.json\"},\n",
    "    )\n",
    "    \n",
    "    # data, labels = parse_file(dataset_folder)\n",
    "    labels = [1 if label == \"same_as\" else 2 if label == \"part_of\" else 3 if label == \"serves\" else 0 if label == \"unknown\" else 5 for label in dataset['test']['answer']]\n",
    "    if test_prompt==\"attribute_value_dist\":\n",
    "        prompts = [prepare_prompt_gtminer_distance(row) for row in dataset['test']]\n",
    "    elif test_prompt==\"simple\":\n",
    "        prompts = [prepare_prompt_gtminer_simple(row) for row in dataset['test']]\n",
    "    else:\n",
    "        prompts = [prepare_prompt_gtminer(row) for row in dataset['test']]\n",
    "    print(prompts[0])\n",
    "    print(dataset['test'][0]['answer'])\n",
    "    predictions = zero_shot_inference(model, tokenizer, prompts, 2)\n",
    "    predictions = [x.split(\" \")[-1].strip() for x in predictions] \n",
    "    predictions = [1 if label in [\"same_as\", \"same\", \"same-as\"] else 2 if label in [\"part_of\", \"part-of\", \"partof\"] else 3 if label in [\"serves\", \"served\"] else 0 if label in [\"unknown\"] else 4 for label in predictions]\n",
    "    print(len(predictions), len(labels))\n",
    "    print(calculate_metrics2(predictions, labels))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T06:33:56.754446900Z",
     "start_time": "2025-02-03T05:12:54.873460700Z"
    }
   },
   "id": "66e4bbbfd4ed9de7",
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
